{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "\n",
    "project_dir = os.path.dirname(os.getcwd())\n",
    "print(project_dir)\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "from hydra import initialize\n",
    "try:\n",
    "    initialize(version_base=None, config_path=\"../sam2_logs/\", job_name=\"predict_run\")\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import torch\n",
    "from torch.amp import autocast\n",
    "from torchvision.transforms import ToPILImage, Normalize, Resize\n",
    "from torchmetrics.detection import PanopticQuality\n",
    "from training.dataset.transforms import ComposeAPI, NormalizeAPI\n",
    "\n",
    "from helpers.configurations import TRACK_TO_METAINFO, LABEL_PROJECTION_MAP\n",
    "from dataset.collate_fn import collate_fn\n",
    "from dataset.mini_dataset import MiniDataset\n",
    "from custom_model_builder import build_sam2former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size_dict = {\n",
    "    # 'base': {\n",
    "    #     'config': '04_28_00_50/config_resolved.yaml',\n",
    "    #     'ck': '/home/guests/tuna_gurbuz/prototype/sam2_logs/04_28_00_50/checkpoints/checkpoint_33.pt',\n",
    "    #     },\n",
    "    # 'base': {\n",
    "    #     'config': '05_24_10_21/config_resolved.yaml',\n",
    "    #     'ck': '/home/guests/tuna_gurbuz/prototype/sam2_logs/05_24_10_21/checkpoints/checkpoint_34.pt',\n",
    "    #     },\n",
    "    'base': {\n",
    "        'config': '06_17_16_27/config_resolved.yaml',\n",
    "        'ck': '/home/guests/tuna_gurbuz/prototype/sam2_logs/06_17_16_27/checkpoints/checkpoint_53.pt',\n",
    "        },\n",
    "}\n",
    "\n",
    "# Model\n",
    "model_size = 'base'\n",
    "config = model_size_dict[model_size]['config']\n",
    "ck = model_size_dict[model_size]['ck']\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "amp_type = torch.bfloat16 if device == 'cuda' else torch.float16\n",
    "submodel, object_labels, _, loss = build_sam2former(config, ck, device=device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(6):\n",
    "    before = submodel.sam_mask_decoder.pixel_decoder.transformer.encoder.layers[ii].self_attn.attention_weights.weight\n",
    "    # print(\"before\", before)\n",
    "    print(torch.all(before == 0))\n",
    "sd = torch.load(ck, map_location=\"cpu\", weights_only=True)[\"model\"]\n",
    "missing_keys, unexpected_keys = submodel.load_state_dict(sd, strict=False)\n",
    "for ii in range(6):\n",
    "    after = submodel.sam_mask_decoder.pixel_decoder.transformer.encoder.layers[ii].self_attn.attention_weights.weight\n",
    "    # print(\"after\", after)\n",
    "    print(torch.all(after == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "len_video = 7\n",
    "input_image_size = 256\n",
    "batch_size = 1\n",
    "shuffle = False\n",
    "mean = [0.3551, 0.3500, 0.3469]\n",
    "std = [0.2921, 0.2716, 0.2742]\n",
    "revert_mean=[-.3551/.2921, -.3500/.2716, -.3469/.2742]\n",
    "revert_std=[1/.2921, 1/.2716, 1/.2742]\n",
    "# mean = [0.485, 0.456, 0.406]\n",
    "# std = [0.229, 0.224, 0.225]\n",
    "# revert_mean=[-.485/.229, -.456/.224, -.406/.225]\n",
    "# revert_std=[1/.229, 1/.224, 1/.225]\n",
    "transforms = [ComposeAPI([NormalizeAPI(mean=mean, std=std, v2=True)])]\n",
    "revert_transform = Normalize(mean=revert_mean, std=revert_std)\n",
    "test_dataset = MiniDataset('test',\n",
    "                           num_frames=len_video,\n",
    "                           input_image_size=input_image_size,\n",
    "                           object_labels=object_labels,\n",
    "                           transforms=transforms,\n",
    "                           collate_fn=collate_fn,\n",
    "                           batch_size=batch_size,\n",
    "                           get_seg_mask=True,\n",
    "                           shuffle=shuffle,)\n",
    "print(f'Lenght of the dataset! {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2 # Check seed 123 index 19966\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Image\n",
    "len_objects = len(object_labels)\n",
    "toPILimage = ToPILImage()\n",
    "exist = False\n",
    "if_break = False\n",
    "# sample_idx = 950\n",
    "# sample_idx = 2000\n",
    "# sample_idx = 1250\n",
    "# # sample_idx = 3993\n",
    "sample_idx = 200\n",
    "test_loader = test_dataset.get_loader()\n",
    "flag_iter = False\n",
    "\n",
    "# Run the model\n",
    "with torch.no_grad():\n",
    "    submodel.eval()\n",
    "    if flag_iter:\n",
    "        batch = next(itertools.islice(test_loader, sample_idx, None))\n",
    "    else:\n",
    "        batch = [test_dataset[i] for i in range(sample_idx * batch_size, (sample_idx+1) * batch_size) if i < len(test_dataset)]\n",
    "        batch = collate_fn(batch, 'val')\n",
    "    batched_video_data_val = batch.to(device)\n",
    "    # batch_seg_mask_gt = batch[1]  # List of PIL Image for debug\n",
    "    masks_val = batch.masks.to(device)\n",
    "    with autocast(device_type=device, dtype=amp_type):\n",
    "        all_frame_outputs_val = submodel(batched_video_data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.images[199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt to new changes (View)\n",
    "all_frame_outputs_val = all_frame_outputs_val[0]\n",
    "\n",
    "# Concatenate frame outputs as batch\n",
    "for i in range(len(all_frame_outputs_val)):\n",
    "    if i == 0:\n",
    "        all_masks = all_frame_outputs_val[i]['pred_masks_high_res']\n",
    "        all_labels = all_frame_outputs_val[i]['pred_logits']\n",
    "    else:\n",
    "        all_masks = torch.cat((all_masks, all_frame_outputs_val[i]['pred_masks_high_res']), dim=0)\n",
    "        all_labels = torch.cat((all_labels, all_frame_outputs_val[i]['pred_logits']), dim=0)\n",
    "\n",
    "# Convert mask prediction logits to binary masks\n",
    "binary_pred_masks = all_masks.sigmoid() > 0.5\n",
    "binary_pred_masks = binary_pred_masks.cpu().numpy()\n",
    "\n",
    "# Convert class logits to class predictions\n",
    "pred_logits = all_labels.type(torch.float32).softmax(-1).cpu().numpy()\n",
    "pred_class = pred_logits.argmax(-1)\n",
    "\n",
    "B, N, H, W = binary_pred_masks.shape  # Size with padding\n",
    "gt_labels = batched_video_data_val.metadata.unique_objects_identifier[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# dump_dict = {k: v.cpu().type(torch.float32).numpy().tolist() for k,v in all_frame_outputs_val[0].items() if k != 'aux_outputs'}\n",
    "# with open('../temp/predictions.json', 'w') as f:\n",
    "#     json.dump(dump_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_map():\n",
    "    color_map = {}\n",
    "    for k, v in LABEL_PROJECTION_MAP.items():\n",
    "        if k > 7 and v['label'] == 6:\n",
    "            continue\n",
    "        r, g, b = v['color']\n",
    "        color_map[v['label']] = (r,g,b)\n",
    "    # color_map = ListedColormap(color_map)\n",
    "    return color_map\n",
    "\n",
    "# Get color map\n",
    "color_map = get_color_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare segmentation mask and RGB mask with color encoding\n",
    "pred_seg_class_mask = np.ones((B, 1, H, W), dtype=np.int8) * 16  # 16 is the background class\n",
    "pred_rgb_mask = np.zeros((B, H, W, 3), dtype=np.uint8)\n",
    "for _b_idx, (_b_binary_pred_masks, _b_pred_class) in enumerate(zip(binary_pred_masks, pred_class)):\n",
    "    for mask, class_id in zip(_b_binary_pred_masks, _b_pred_class):\n",
    "        pos = np.where(mask == True)\n",
    "        if len(pos[0]) > 0:\n",
    "            pred_seg_class_mask[_b_idx, 0, pos[0], pos[1]] = class_id\n",
    "            pred_rgb_mask[_b_idx, pos[0], pos[1], :] = color_map[class_id]\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_video_data_val.obj_to_frame_idx.shape, batched_video_data_val.obj_to_frame_idx[:,:,0], masks_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_seg_class_mask = np.ones((B, 1, H, W), dtype=np.int8) * 16  # 16 is the background class\n",
    "gt_rgb_mask = np.zeros((B, H, W, 3), dtype=np.uint8)\n",
    "for batch_idx in range(B):\n",
    "    pos = batched_video_data_val.obj_to_frame_idx[:,:,0] == batch_idx  # The objects in the batch are mixed\n",
    "    gt_mask = masks_val[pos, :, :].cpu().numpy()  # Get the GT masks in the batch\n",
    "    gt_class_id = gt_labels[pos].cpu().numpy()  # Get the GT class id in the batch\n",
    "    for mask, class_id in zip(gt_mask, gt_class_id):\n",
    "        pos = np.where(mask == True)\n",
    "        if len(pos[0]) > 0:\n",
    "            gt_seg_class_mask[batch_idx, 0, pos[0], pos[1]] = class_id\n",
    "            gt_rgb_mask[batch_idx, pos[0], pos[1], :] = color_map[class_id]\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for masks\n",
    "pred_rgb_mask_list_azure = []\n",
    "pred_seg_class_mask_list_azure = []\n",
    "gt_rgb_mask_list_azure = []\n",
    "gt_seg_class_mask_list_azure = []\n",
    "\n",
    "pred_rgb_mask_list_sim = []  # Auxillary lists for now\n",
    "pred_seg_class_mask_list_sim = []  # Auxillary lists for now\n",
    "gt_rgb_mask_list_sim = []  # Auxillary lists for now\n",
    "gt_seg_class_mask_list_sim = []  # Auxillary lists for now\n",
    "\n",
    "# Revert the padding\n",
    "for batch_idx in range(B):\n",
    "    # Load the corresponding rgb image to look for padding \n",
    "    img = batched_video_data_val.img_batch[batch_idx][0].permute(1,2,0).cpu().numpy()\n",
    "    \n",
    "    # Check if the image is black (0,0,0) along the height dimension\n",
    "    is_black_row = (img[:,:,0] <= 0)  # Check along width and channels\n",
    "    pos_padding = np.where(is_black_row == False)\n",
    "    start = pos_padding[0][0]\n",
    "    end = pos_padding[0][-1]\n",
    "    \n",
    "    # Slice the predictions and ground truth masks\n",
    "    if start == 32:\n",
    "        pred_rgb_mask_list_azure.append(pred_rgb_mask[batch_idx, start:end+1, :, :])\n",
    "        pred_seg_class_mask_list_azure.append(pred_seg_class_mask[batch_idx, 0:1, start:end+1, :])\n",
    "        gt_rgb_mask_list_azure.append(gt_rgb_mask[batch_idx, start:end+1, :, :])\n",
    "        gt_seg_class_mask_list_azure.append(gt_seg_class_mask[batch_idx, 0:1, start:end+1, :])\n",
    "    elif start == 0:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Padding not found in the image\")\n",
    "\n",
    "# Concatenate the masks\n",
    "pred_rgb_mask = np.array(pred_rgb_mask_list_azure)\n",
    "pred_seg_class_mask = np.array(pred_seg_class_mask_list_azure)\n",
    "gt_rgb_mask = np.array(gt_rgb_mask_list_azure)\n",
    "gt_seg_class_mask = np.array(gt_seg_class_mask_list_azure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color encoded segmentation masks and label encoded segmentation masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = np.array([4, B]) * 3\n",
    "fig, axs = plt.subplots(B, 4, figsize=figsize, )\n",
    "\n",
    "for ii in range(B):\n",
    "    axs[ii,0].imshow(pred_rgb_mask[ii])\n",
    "    axs[ii,0].axis('off')\n",
    "    axs[ii,0].set_title('PRED')\n",
    "\n",
    "    axs[ii,1].imshow(gt_rgb_mask[ii])\n",
    "    axs[ii,1].axis('off')\n",
    "    axs[ii,1].set_title('GT')\n",
    "\n",
    "    axs[ii,2].imshow(pred_seg_class_mask[ii, 0])\n",
    "    axs[ii,2].axis('off')\n",
    "    axs[ii,2].set_title('PRED')\n",
    "\n",
    "    axs[ii,3].imshow(gt_seg_class_mask[ii, 0])\n",
    "    axs[ii,3].axis('off')\n",
    "    axs[ii,3].set_title('GT')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Panoptic Quality (PQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuff: 16, Things: 17 Instance labels: 0-15\n",
    "background = 16\n",
    "things = {LABEL_PROJECTION_MAP[idx]['label'] for idx in object_labels}\n",
    "stuff = {background, background+1}  # Putting 17 does not change anything for the last value\n",
    "print(f'Things: {things}\\nStuff: {stuff}')\n",
    "\n",
    "# Prepare stuff and things mask for prediction\n",
    "preds = pred_seg_class_mask[:, 0, :, :, None]\n",
    "stuff_things_mask = np.ones_like(preds) * (background + 1)  # 17 is the stuff label\n",
    "pos_stuff = np.where(preds[:, :, :, 0] == background)  # Find where the background is\n",
    "stuff_things_mask[pos_stuff[0], pos_stuff[1], pos_stuff[2], 0] = background  # Set the stuff label background\n",
    "preds = np.concatenate((stuff_things_mask, preds), axis=3)\n",
    "\n",
    "# Prepare stuff and things mask for GT\n",
    "target = gt_seg_class_mask[:, 0, :, :, None]\n",
    "stuff_things_mask_gt = np.ones_like(target) * (background + 1)  # 17 is the stuff label\n",
    "pos_stuff = np.where(target[:, :, :, 0] == background)  # Find where the background is\n",
    "stuff_things_mask_gt[pos_stuff[0], pos_stuff[1], pos_stuff[2], 0] = background  # Set the stuff label background\n",
    "target = np.concatenate((stuff_things_mask_gt, target), axis=3)\n",
    "\n",
    "# Convert to torch tensors\n",
    "preds = torch.tensor(preds)\n",
    "target = torch.tensor(target)\n",
    "\n",
    "# Prepare panoptic quality metric\n",
    "panoptic_quality = PanopticQuality(stuffs=stuff, things=things)\n",
    "pq = panoptic_quality(preds, target)\n",
    "print(f'Prediction: {preds.shape}, Target: {target.shape}')\n",
    "print(f\"PQ: {pq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Masks on Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batched_video_data_val.img_batch.shape = (N, B, C, H, W)  \n",
    "masks_val.shape = (N, B*O, H, W)  \n",
    "metadata.unique_objects_identifier = (N, B*O, 3) => [video_id, object_id, frame_id]  \n",
    "obj_to_frame_idx = (N, B*O, 2) => [frame_id, video_id]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_map():\n",
    "    color_map = []\n",
    "    for k, v in LABEL_PROJECTION_MAP.items():\n",
    "        if k > 7 and v['label'] == 6:\n",
    "            continue\n",
    "        r, g, b = v['color']\n",
    "        color_map.append(sv.Color(r,g,b))\n",
    "    color_map = sv.ColorPalette(colors=color_map)\n",
    "    return color_map\n",
    "\n",
    "# Get color map\n",
    "color_map = get_color_map()\n",
    "\n",
    "# GT Labels\n",
    "gt_labels = batched_video_data_val.metadata.unique_objects_identifier[:,:,1]\n",
    "\n",
    "# Create subplots\n",
    "figsize = np.array([2, B]) * 4\n",
    "fig, axs = plt.subplots(B, 2, figsize=figsize, )\n",
    "\n",
    "for batch_idx in range(B):\n",
    "    # Unnormalize the image\n",
    "    image = batched_video_data_val.img_batch[batch_idx][0]\n",
    "    unnormalized_image = toPILimage(revert_transform(image))\n",
    "\n",
    "    # GT masks\n",
    "    # Assumption: Batch size is 1, by batch index we mean frames\n",
    "    gt_mask = masks_val[batch_idx, :, :, :].cpu().numpy()\n",
    "    gt_class_id = gt_labels[batch_idx].cpu().numpy()\n",
    "    empty_bboxes = np.array([[0, 0, 0, 0]] * len(gt_class_id))\n",
    "\n",
    "    # Gt Annotated Frame \n",
    "    gt_detections = sv.Detections(xyxy=empty_bboxes, mask=gt_mask, class_id=gt_class_id)\n",
    "    mask_annotator = sv.MaskAnnotator(color=color_map)\n",
    "    gt_annotated_frame = mask_annotator.annotate(\n",
    "        scene=unnormalized_image.copy(),\n",
    "        detections=gt_detections\n",
    "    )\n",
    "    # sv.plot_image(gt_annotated_frame, size=(4,4)) if B <= 10 else None\n",
    "\n",
    "    # Prediction masks\n",
    "    pred_mask = (all_frame_outputs_val[batch_idx]['pred_masks_high_res'][0].sigmoid() > 0.5).cpu().numpy()\n",
    "    # pred_class_ids = best_probs.indices.numpy().astype(np.int32)\n",
    "    empty_bboxes = np.array([[0, 0, 0, 0]] * len(pred_class[0]))\n",
    "\n",
    "    # Prediction Annotated Frame\n",
    "    pred_detections = sv.Detections(xyxy=empty_bboxes, mask=pred_mask, class_id=pred_class[batch_idx])\n",
    "    mask_annotator = sv.MaskAnnotator(color=color_map)\n",
    "    pred_annotated_frame = mask_annotator.annotate(\n",
    "        scene=unnormalized_image.copy(),\n",
    "        detections=pred_detections\n",
    "    )\n",
    "    # sv.plot_image(pred_annotated_frame, size=(4,4)) if B <= 10 else None\n",
    "    axs[batch_idx,0].imshow(pred_annotated_frame, )\n",
    "    axs[batch_idx,0].axis('off')\n",
    "    axs[batch_idx,0].set_title(f'PRED w PQ: {pq:.4f}')\n",
    "\n",
    "    axs[batch_idx,1].imshow(gt_annotated_frame)\n",
    "    axs[batch_idx,1].axis('off')\n",
    "    axs[batch_idx,1].set_title('GT')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig('../temp/pred_gt.png', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_mask = masks_val[batch_idx].cpu()\n",
    "# black_placeholder = np.zeros_like(gt_mask[0])  # Black image placeholder\n",
    "\n",
    "# # Create figure and subplots\n",
    "# fig, axes = plt.subplots(4, 3, figsize=(10, 6))\n",
    "\n",
    "# # First row (RGB, GT, Black Placeholder)\n",
    "# axes[0, 0].imshow(unnormalized_image)\n",
    "# axes[0, 0].set_title(\"RGB Image\")\n",
    "\n",
    "# axes[0, 1].imshow(black_placeholder, cmap='gray')\n",
    "# axes[0, 1].set_title(\"Placeholder\")\n",
    "\n",
    "# axes[0, 2].imshow(gt_mask[0], cmap='gray')\n",
    "# axes[0, 2].set_title(\"GT Mask 0\")\n",
    "\n",
    "# # Second row (GT gt_mask)\n",
    "# axes[1, 0].imshow(gt_mask[1], cmap='gray')\n",
    "# axes[1, 0].set_title(\"GT Mask 1\")\n",
    "\n",
    "# axes[1, 1].imshow(gt_mask[2], cmap='gray')\n",
    "# axes[1, 1].set_title(\"GT Mask 2\")\n",
    "\n",
    "# axes[1, 2].imshow(gt_mask[3], cmap='gray')\n",
    "# axes[1, 2].set_title(\"GT Mask 3\")\n",
    "\n",
    "# # Third row (GT gt_mask)\n",
    "# axes[2, 0].imshow(gt_mask[4], cmap='gray')\n",
    "# axes[2, 0].set_title(\"GT Mask 4\")\n",
    "\n",
    "# axes[2, 1].imshow(gt_mask[5], cmap='gray')\n",
    "# axes[2, 1].set_title(\"GT Mask 5\")\n",
    "\n",
    "# axes[2, 2].imshow(gt_mask[6], cmap='gray')\n",
    "# axes[2, 2].set_title(\"GT Mask 6\")\n",
    "\n",
    "# # Fourth row (GT gt_mask)\n",
    "# axes[3, 0].imshow(gt_mask[7], cmap='gray')\n",
    "# axes[3, 0].set_title(\"GT Mask 7\")\n",
    "\n",
    "# axes[3, 1].imshow(gt_mask[8], cmap='gray')\n",
    "# axes[3, 1].set_title(\"GT Mask 8\")\n",
    "\n",
    "# axes[3, 2].imshow(gt_mask[9], cmap='gray')\n",
    "# axes[3, 2].set_title(\"GT Mask 9\")\n",
    "\n",
    "# # Remove axes for a cleaner look\n",
    "# for ax in axes.ravel():\n",
    "#     ax.axis('off')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# black_placeholder = np.zeros_like(gt_mask[0])  # Black image placeholder\n",
    "# pred_mask = (all_frame_outputs_val[batch_idx]['pred_masks'][0].sigmoid() > 0.5).bool().numpy()\n",
    "\n",
    "# # Create figure and subplots\n",
    "# fig, axes = plt.subplots(4, 3, figsize=(10, 6))\n",
    "\n",
    "# # First row (RGB, GT, Black Placeholder)\n",
    "# axes[0, 0].imshow(unnormalized_image)\n",
    "# axes[0, 0].set_title(\"RGB Image\")\n",
    "\n",
    "# axes[0, 1].imshow(black_placeholder, cmap='gray')\n",
    "# axes[0, 1].set_title(\"Placeholder\")\n",
    "\n",
    "# axes[0, 2].imshow(pred_mask[0], cmap='gray')\n",
    "# axes[0, 2].set_title(\"Prediction 0\")\n",
    "\n",
    "# # Second row (Predictions)\n",
    "# axes[1, 0].imshow(pred_mask[1], cmap='gray')\n",
    "# axes[1, 0].set_title(\"Prediction 1\")\n",
    "\n",
    "# axes[1, 1].imshow(pred_mask[2], cmap='gray')\n",
    "# axes[1, 1].set_title(\"Prediction 2\")\n",
    "\n",
    "# axes[1, 2].imshow(pred_mask[3], cmap='gray')\n",
    "# axes[1, 2].set_title(\"Prediction 3\")\n",
    "\n",
    "# # Third row (Predictions)\n",
    "# axes[2, 0].imshow(pred_mask[4], cmap='gray')\n",
    "# axes[2, 0].set_title(\"Prediction 4\")\n",
    "\n",
    "# axes[2, 1].imshow(pred_mask[5], cmap='gray')\n",
    "# axes[2, 1].set_title(\"Prediction 5\")\n",
    "\n",
    "# axes[2, 2].imshow(pred_mask[6], cmap='gray')\n",
    "# axes[2, 2].set_title(\"Prediction 6\")\n",
    "\n",
    "# # Fourth row (Predictions)\n",
    "# axes[3, 0].imshow(pred_mask[7], cmap='gray')\n",
    "# axes[3, 0].set_title(\"Prediction 7\")\n",
    "\n",
    "# axes[3, 1].imshow(pred_mask[8], cmap='gray')\n",
    "# axes[3, 1].set_title(\"Prediction 8\")\n",
    "\n",
    "# axes[3, 2].imshow(pred_mask[9], cmap='gray')\n",
    "# axes[3, 2].set_title(\"Prediction 9\")\n",
    "\n",
    "# # Remove axes for a cleaner look\n",
    "# for ax in axes.ravel():\n",
    "#     ax.axis('off')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
