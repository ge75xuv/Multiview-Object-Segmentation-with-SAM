{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "\n",
    "project_dir = os.path.dirname(os.getcwd())\n",
    "print(project_dir)\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "from hydra import initialize\n",
    "try:\n",
    "    initialize(version_base=None, config_path=\"../sam2_logs/\", job_name=\"predict_run\")\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import supervision as sv\n",
    "import torch\n",
    "from torch.amp import autocast\n",
    "from torchvision.transforms import ToPILImage, ToTensor, Normalize\n",
    "from training.dataset.transforms import ComposeAPI, NormalizeAPI\n",
    "\n",
    "from helpers.configurations import TRACK_TO_METAINFO, LABEL_PROJECTION_MAP\n",
    "from dataset.collate_fn import collate_fn\n",
    "from dataset.mini_dataset import MiniDataset\n",
    "from custom_model_builder import build_sam2former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size_dict = {\n",
    "    'base': {\n",
    "        'config': '04_02_14_02/config_resolved.yaml',\n",
    "        'ck': '/home/guests/tuna_gurbuz/prototype/sam2_logs/04_02_14_02/checkpoints/checkpoint_200.pt',\n",
    "        },\n",
    "}\n",
    "\n",
    "# Model\n",
    "model_size = 'base'\n",
    "config = model_size_dict[model_size]['config']\n",
    "ck = model_size_dict[model_size]['ck']\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "amp_type = torch.bfloat16 if device == 'cuda' else torch.float16\n",
    "submodel, object_labels, _, loss = build_sam2former(config, ck, device=device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(6):\n",
    "    before = submodel.sam_mask_decoder.pixel_decoder.transformer.encoder.layers[ii].self_attn.attention_weights.weight\n",
    "    print(\"before\", before)\n",
    "    print(torch.all(before == 0))\n",
    "sd = torch.load(ck, map_location=\"cpu\", weights_only=True)[\"model\"]\n",
    "missing_keys, unexpected_keys = submodel.load_state_dict(sd, strict=False)\n",
    "for ii in range(6):\n",
    "    after = submodel.sam_mask_decoder.pixel_decoder.transformer.encoder.layers[ii].self_attn.attention_weights.weight\n",
    "    print(\"after\", after)\n",
    "    print(torch.all(after == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "len_video = 1\n",
    "input_image_size = 512\n",
    "batch_size = 1\n",
    "shuffle = False\n",
    "mean = [0.3551, 0.3500, 0.3469]\n",
    "std = [0.2921, 0.2716, 0.2742]\n",
    "revert_mean=[-.3551/.2921, -.3500/.2716, -.3469/.2742]\n",
    "revert_std=[1/.2921, 1/.2716, 1/.2742]\n",
    "# mean = [0.485, 0.456, 0.406]\n",
    "# std = [0.229, 0.224, 0.225]\n",
    "# revert_mean=[-.485/.229, -.456/.224, -.406/.225]\n",
    "# revert_std=[1/.229, 1/.224, 1/.225]\n",
    "transforms = [ComposeAPI([NormalizeAPI(mean=mean, std=std, v2=True)])]\n",
    "revert_transform = Normalize(mean=revert_mean, std=revert_std)\n",
    "test_dataset = MiniDataset('val',\n",
    "                           len_video=len_video,\n",
    "                           input_image_size=input_image_size,\n",
    "                           object_labels=object_labels,\n",
    "                           transforms=transforms,\n",
    "                           collate_fn=collate_fn,\n",
    "                           batch_size=batch_size,\n",
    "                           get_seg_mask=True)\n",
    "print(f'Lenght of the dataset! {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2 # Check seed 123 index 19966\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Image\n",
    "len_objects = len(object_labels)\n",
    "toPILimage = ToPILImage()\n",
    "exist = False\n",
    "if_break = False\n",
    "sample_idx = 3\n",
    "test_loader = test_dataset.get_loader()\n",
    "with torch.no_grad():\n",
    "    submodel.train()\n",
    "    for idx, batch in enumerate(test_loader):\n",
    "        if idx < sample_idx:\n",
    "            continue\n",
    "        elif idx > sample_idx:\n",
    "            break\n",
    "        batched_video_data_val = batch[0].to(device)\n",
    "        seg_mask = batch[1]  # List of PIL Image for debug\n",
    "        masks_val = batch[0].masks.to(device)\n",
    "        with autocast(device_type=device, dtype=amp_type):\n",
    "            all_frame_outputs_val = submodel(batched_video_data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image\n",
    "image = batched_video_data_val.img_batch[0,0].cpu()\n",
    "unnormalized_image = toPILimage(revert_transform(image))\n",
    "# GT Mask\n",
    "gt_mask = masks_val[0,0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch[0]\n",
    "batch_size = len(batch.img_batch)\n",
    "key = batch.dict_key  # key for dataset\n",
    "targets = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    # dim=2 video_id, obj_id, frame_id\n",
    "    xx, yy = torch.where(batch.metadata.unique_objects_identifier[:,:,2] == i)\n",
    "    obj_id = batch.metadata.unique_objects_identifier[xx,yy,1]\n",
    "    targets.append({\n",
    "        \"masks\": batch.masks[i],\n",
    "        \"labels\": obj_id,\n",
    "    })\n",
    "    \n",
    "loss[key](all_frame_outputs_val, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted Mask\n",
    "pred_logits = all_frame_outputs_val[0]['pred_logits'][0].cpu()\n",
    "best_probs = pred_logits.softmax(dim=-1).max(dim=-1)\n",
    "print(\"Predicted class probabilty\", best_probs)\n",
    "bin_pred_masks = (all_frame_outputs_val[0]['pred_masks'].sigmoid() > 0.5).float()\n",
    "masks = bin_pred_masks[0].cpu()\n",
    "gt_labels = batch.metadata.unique_objects_identifier[:,:,1]\n",
    "print(\"GT labels\", gt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_map():\n",
    "    color_map = []\n",
    "    for k, v in LABEL_PROJECTION_MAP.items():\n",
    "        r, g, b = v['color']\n",
    "        color_map.append(sv.Color(r,g,b))\n",
    "    color_map = sv.ColorPalette(colors=color_map)\n",
    "    return color_map\n",
    "\n",
    "# Get color map\n",
    "color_map = get_color_map()\n",
    "\n",
    "# GT masks\n",
    "gt_mask = masks_val[0].cpu().numpy()\n",
    "gt_class_id = gt_labels[0].cpu().numpy()\n",
    "empty_bboxes = np.array([[0, 0, 0, 0]] * len(gt_class_id))\n",
    "\n",
    "# Gt Annotated Frame \n",
    "gt_detections = sv.Detections(xyxy=empty_bboxes, mask=gt_mask, class_id=gt_class_id)\n",
    "mask_annotator = sv.MaskAnnotator(color=color_map)\n",
    "gt_annotated_frame = mask_annotator.annotate(\n",
    "    scene=np.array(unnormalized_image),\n",
    "    detections=gt_detections\n",
    ")\n",
    "sv.plot_image(gt_annotated_frame)\n",
    "\n",
    "# Prediction masks\n",
    "pred_mask = (all_frame_outputs_val[0]['pred_masks_high_res'][0].sigmoid() > 0.5).bool().numpy()\n",
    "pred_class_ids = best_probs.indices.numpy().astype(np.int32)\n",
    "empty_bboxes = np.array([[0, 0, 0, 0]] * len(pred_class_ids))\n",
    "\n",
    "# Prediction Annotated Frame\n",
    "pred_detections = sv.Detections(xyxy=empty_bboxes, mask=pred_mask, class_id=pred_class_ids)\n",
    "mask_annotator = sv.MaskAnnotator(color=color_map)\n",
    "pred_annotated_frame = mask_annotator.annotate(\n",
    "    scene=np.array(unnormalized_image),\n",
    "    detections=pred_detections\n",
    ")\n",
    "sv.plot_image(pred_annotated_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_out_idx = 0\n",
    "gt_mask = masks_val[0].cpu()\n",
    "black_placeholder = np.zeros_like(gt_mask[0])  # Black image placeholder\n",
    "\n",
    "# Create figure and subplots\n",
    "fig, axes = plt.subplots(4, 3, figsize=(10, 6))\n",
    "\n",
    "# First row (RGB, GT, Black Placeholder)\n",
    "axes[0, 0].imshow(unnormalized_image)\n",
    "axes[0, 0].set_title(\"RGB Image\")\n",
    "\n",
    "axes[0, 1].imshow(black_placeholder, cmap='gray')\n",
    "axes[0, 1].set_title(\"Placeholder\")\n",
    "\n",
    "axes[0, 2].imshow(gt_mask[0], cmap='gray')\n",
    "axes[0, 2].set_title(\"GT Mask 0\")\n",
    "\n",
    "# Second row (GT gt_mask)\n",
    "axes[1, 0].imshow(gt_mask[1], cmap='gray')\n",
    "axes[1, 0].set_title(\"GT Mask 1\")\n",
    "\n",
    "axes[1, 1].imshow(gt_mask[2], cmap='gray')\n",
    "axes[1, 1].set_title(\"GT Mask 2\")\n",
    "\n",
    "axes[1, 2].imshow(gt_mask[3], cmap='gray')\n",
    "axes[1, 2].set_title(\"GT Mask 3\")\n",
    "\n",
    "# Third row (GT gt_mask)\n",
    "axes[2, 0].imshow(gt_mask[4], cmap='gray')\n",
    "axes[2, 0].set_title(\"GT Mask 4\")\n",
    "\n",
    "axes[2, 1].imshow(gt_mask[5], cmap='gray')\n",
    "axes[2, 1].set_title(\"GT Mask 5\")\n",
    "\n",
    "axes[2, 2].imshow(gt_mask[6], cmap='gray')\n",
    "axes[2, 2].set_title(\"GT Mask 6\")\n",
    "\n",
    "# Fourth row (GT gt_mask)\n",
    "axes[3, 0].imshow(gt_mask[7], cmap='gray')\n",
    "axes[3, 0].set_title(\"GT Mask 7\")\n",
    "\n",
    "axes[3, 1].imshow(gt_mask[8], cmap='gray')\n",
    "axes[3, 1].set_title(\"GT Mask 8\")\n",
    "\n",
    "axes[3, 2].imshow(gt_mask[9], cmap='gray')\n",
    "axes[3, 2].set_title(\"GT Mask 9\")\n",
    "\n",
    "# Remove axes for a cleaner look\n",
    "for ax in axes.ravel():\n",
    "    ax.axis('off')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_out_idx = 0\n",
    "black_placeholder = np.zeros_like(gt_mask[0])  # Black image placeholder\n",
    "\n",
    "# Create figure and subplots\n",
    "fig, axes = plt.subplots(4, 3, figsize=(10, 6))\n",
    "\n",
    "# First row (RGB, GT, Black Placeholder)\n",
    "axes[0, 0].imshow(unnormalized_image)\n",
    "axes[0, 0].set_title(\"RGB Image\")\n",
    "\n",
    "axes[0, 1].imshow(black_placeholder, cmap='gray')\n",
    "axes[0, 1].set_title(\"Placeholder\")\n",
    "\n",
    "axes[0, 2].imshow(masks[0], cmap='gray')\n",
    "axes[0, 2].set_title(\"Prediction 0\")\n",
    "\n",
    "# Second row (Predictions)\n",
    "axes[1, 0].imshow(masks[1], cmap='gray')\n",
    "axes[1, 0].set_title(\"Prediction 1\")\n",
    "\n",
    "axes[1, 1].imshow(masks[2], cmap='gray')\n",
    "axes[1, 1].set_title(\"Prediction 2\")\n",
    "\n",
    "axes[1, 2].imshow(masks[3], cmap='gray')\n",
    "axes[1, 2].set_title(\"Prediction 3\")\n",
    "\n",
    "# Third row (Predictions)\n",
    "axes[2, 0].imshow(masks[4], cmap='gray')\n",
    "axes[2, 0].set_title(\"Prediction 4\")\n",
    "\n",
    "axes[2, 1].imshow(masks[5], cmap='gray')\n",
    "axes[2, 1].set_title(\"Prediction 5\")\n",
    "\n",
    "axes[2, 2].imshow(masks[6], cmap='gray')\n",
    "axes[2, 2].set_title(\"Prediction 6\")\n",
    "\n",
    "# Fourth row (Predictions)\n",
    "axes[3, 0].imshow(masks[7], cmap='gray')\n",
    "axes[3, 0].set_title(\"Prediction 7\")\n",
    "\n",
    "axes[3, 1].imshow(masks[8], cmap='gray')\n",
    "axes[3, 1].set_title(\"Prediction 8\")\n",
    "\n",
    "axes[3, 2].imshow(masks[9], cmap='gray')\n",
    "axes[3, 2].set_title(\"Prediction 9\")\n",
    "\n",
    "# Remove axes for a cleaner look\n",
    "for ax in axes.ravel():\n",
    "    ax.axis('off')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
