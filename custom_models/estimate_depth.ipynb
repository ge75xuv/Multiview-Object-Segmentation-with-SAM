{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd36654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "import argparse\n",
    "\n",
    "project_dir = os.path.dirname(os.getcwd())\n",
    "print(project_dir)\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "import depth_pro\n",
    "import diffusers\n",
    "from diffusers import DDIMScheduler, MarigoldDepthPipeline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import ToPILImage, ToTensor, Normalize\n",
    "from training.dataset.transforms import ComposeAPI, NormalizeAPI\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset.collate_fn import collate_fn\n",
    "from dataset.mini_dataset import MiniDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40701ef2",
   "metadata": {},
   "source": [
    "DATASET IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba32565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "object_labels = [13, 15, 16]\n",
    "len_video = 1\n",
    "input_image_size = 512\n",
    "batch_size = 1\n",
    "multiview = True\n",
    "shuffle = False\n",
    "mean = [0.3551, 0.3500, 0.3469]\n",
    "std = [0.2921, 0.2716, 0.2742]\n",
    "transforms = [ComposeAPI([NormalizeAPI(mean=mean, std=std, v2=True)])]\n",
    "revert_mean=[-.3551/.2921, -.3500/.2716, -.3469/.2742]\n",
    "revert_std=[1/.2921, 1/.2716, 1/.2742]\n",
    "revert_transform = Normalize(mean=revert_mean, std=revert_std)\n",
    "test_dataset = MiniDataset('train',\n",
    "                           num_frames=len_video,\n",
    "                           input_image_size=input_image_size,\n",
    "                           object_labels=object_labels,\n",
    "                           transforms=transforms,\n",
    "                           multiview=multiview,\n",
    "                           collate_fn=collate_fn,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=shuffle,\n",
    "                           get_seg_mask=True)\n",
    "print(f'Lenght of the dataset! {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07368a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123  # Check seed 123 index 19966\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "# 035 idx 2761 has a problem!\n",
    "# Image\n",
    "len_objects = len(object_labels)\n",
    "toPILimage = ToPILImage()\n",
    "idx = 1442\n",
    "\n",
    "file_name = test_dataset.images[idx][0]  # Get the file name from the dataset, list to string\n",
    "print(f'File name: {file_name}')\n",
    "frame_obj_list, _ = test_dataset[idx]\n",
    "if multiview:\n",
    "    figsize = (24, 8)\n",
    "    fig, axs = plt.subplots(1, 3, figsize=figsize)\n",
    "    im_list = []\n",
    "    for i in range(3):\n",
    "        image = frame_obj_list[i].frames[0].data\n",
    "        image = revert_transform(image).permute(1, 2, 0).cpu().numpy()\n",
    "        im_list.append(image)\n",
    "        axs[i].imshow(image)\n",
    "        axs[i].axis('off')\n",
    "else:\n",
    "    image = frame_obj_list.frames[0].data\n",
    "    image = revert_transform(image)\n",
    "    image = toPILimage(image)\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f227c38f",
   "metadata": {},
   "source": [
    "CAMERA MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e548ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_camera_data(camera_data, downscale=1):\n",
    "    h, w = camera_data['value0']['color_parameters']['height'], camera_data['value0']['color_parameters']['width']\n",
    "    h_, w_ = h // downscale, w // downscale\n",
    "    padding = (w_ - h_) // 2\n",
    "    intrinsics_json = camera_data['value0']['color_parameters']['intrinsics_matrix']\n",
    "    K = np.asarray([[intrinsics_json['m00'] / downscale, intrinsics_json['m10'], intrinsics_json['m20'] / downscale],\n",
    "                    [intrinsics_json['m01'], intrinsics_json['m11'] / downscale, intrinsics_json['m21'] / downscale + padding],\n",
    "                    [0, 0, 1]])\n",
    "\n",
    "    return K, padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "downscale = 4\n",
    "take_path = file_name.parent.parent  # Get the parent directory of the file\n",
    "# take_path = '/home/polyaxon-data/data1/MM-OR_processed/002_PKA/'\n",
    "camera_files = ['camera01.json', 'camera04.json', 'camera05.json']\n",
    "camera_int_ext = {'camera01': [], 'camera04': [], 'camera05': []}\n",
    "for json_file in camera_files:\n",
    "    with open(os.path.join(take_path, json_file), 'r') as f:\n",
    "        camera_data = json.load(f)\n",
    "    intr, padding = load_camera_data(camera_data, downscale)\n",
    "    dict_key = json_file.split('.')[0]\n",
    "    camera_int_ext[dict_key].append(intr)\n",
    "\n",
    "camera_file = str(file_name).split('/')[-1].split('_')[0]\n",
    "K = camera_int_ext[camera_file][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a32819c",
   "metadata": {},
   "source": [
    "GROUND TRUTH DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93202a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if multiview:\n",
    "    for i in range(3):\n",
    "        depth_image_path = take_path / 'depthimage'\n",
    "        d_path = depth_image_path / str(file_name).split('/')[-1].replace('.jpg', '.tiff').replace('color', 'depth')\n",
    "        print(f'Depth Path: {d_path}')\n",
    "        # d_path = depth_image_path / f'camera0{c_idx}_depthimage-{azure_idx_str}.tiff'\n",
    "        gt_depth_image = Image.open(d_path)\n",
    "        gt_depth_image = gt_depth_image.resize((gt_depth_image.size[0]//2, gt_depth_image.size[1]//2))\n",
    "        gt_depth_image = np.array(gt_depth_image)\n",
    "else:\n",
    "    depth_image_path = take_path / 'depthimage'\n",
    "    d_path = depth_image_path / str(file_name).split('/')[-1].replace('.jpg', '.tiff').replace('color', 'depth')\n",
    "    print(f'Depth Path: {d_path}')\n",
    "    # d_path = depth_image_path / f'camera0{c_idx}_depthimage-{azure_idx_str}.tiff'\n",
    "    gt_depth_image = Image.open(d_path)\n",
    "    gt_depth_image = gt_depth_image.resize((gt_depth_image.size[0]//2, gt_depth_image.size[1]//2))\n",
    "    gt_depth_image = np.array(gt_depth_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79501471",
   "metadata": {},
   "source": [
    "DEPTH PRO PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6e47cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and preprocessing transform\n",
    "config = depth_pro.DEFAULT_MONODEPTH_CONFIG_DICT  # Changed init for this\n",
    "config.checkpoint_uri = '/home/guests/tuna_gurbuz/prototype/models/ml-depth-pro/checkpoints/depth_pro.pt'\n",
    "model, transform = depth_pro.create_model_and_transforms()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove padding from the image\n",
    "image_cropped = ToTensor()(image)\n",
    "image_cropped = image_cropped[:, padding:-padding, :]\n",
    "image_cropped = toPILimage(image_cropped)\n",
    "\n",
    "# Preprocess an image\n",
    "image_transformed = transform(image_cropped)\n",
    "f_px = torch.tensor(K[0, 0])  # Focal length in pixels (fx)\n",
    "\n",
    "# Run inference\n",
    "if True:\n",
    "    depth = np.zeros_like(gt_depth_image, dtype=np.float32)\n",
    "    depth = torch.tensor(depth)\n",
    "else:\n",
    "    prediction = model.infer(image_transformed, f_px=f_px)\n",
    "    depth = prediction[\"depth\"]  # Depth in [mm].\n",
    "    focallength_px = prediction[\"focallength_px\"]  # Focal length in pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = np.where(gt_depth_image <= 100)\n",
    "# test_gt_depth_image = deepcopy(gt_depth_image)\n",
    "# test_gt_depth_image[pos] = 0\n",
    "# np.save('test_gt_depth_image.npy', test_gt_depth_image)\n",
    "# image_cropped.save('../temp/test_image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ebdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "axes[0].imshow(depth.cpu().numpy(), cmap='plasma')\n",
    "axes[0].set_title('Predicted Depth')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(gt_depth_image, cmap='plasma')\n",
    "axes[1].set_title('Ground Truth Depth')\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e21bb98",
   "metadata": {},
   "source": [
    "Depth Scaling $\\theta = argmin ||Y - X\\theta||_2^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d15312",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.where(gt_depth_image >= 100)  # 100 mm threshold\n",
    "gt_y = gt_depth_image[pos][:, None] / 1000.0  # Convert to meters\n",
    "x = depth.cpu().numpy()[pos][:, None]\n",
    "\n",
    "# Linear regression to find the scale factor\n",
    "scaling_factor = np.linalg.inv(x.T @ x) @ (x.T @ gt_y)  # It is scaler so leave the np.eye\n",
    "print(f'Scaling factor: {scaling_factor}')\n",
    "\n",
    "# Apply scaling factor to the predicted depth\n",
    "depth_scaled = depth.cpu().numpy() * scaling_factor[0, 0]\n",
    "\n",
    "# Check MSE\n",
    "se = (x * scaling_factor - gt_y) ** 2\n",
    "mse = np.mean(se)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Max Error: {se.max()}')\n",
    "pos_max = se.argmax()\n",
    "print(f'Max Error Position: {pos_max}, Predicted: {x[pos_max]}, GT: {gt_y[pos_max]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bab1814",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "axes[0].imshow(depth_scaled, cmap='plasma')\n",
    "axes[0].set_title('Predicted Depth')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(gt_depth_image, cmap='plasma')\n",
    "axes[1].set_title('Ground Truth Depth')\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e57ea1",
   "metadata": {},
   "source": [
    "Depth Prediction with Marigold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362102cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "diffusers.utils.logging.disable_progress_bar()\n",
    "\n",
    "class MarigoldDepthCompletionPipeline(MarigoldDepthPipeline):\n",
    "    \"\"\"\n",
    "    Pipeline for Marigold Depth Completion.\n",
    "    Extends the MarigoldDepthPipeline to include depth completion functionality.\n",
    "    \"\"\"\n",
    "    def __call__(\n",
    "        self, image: Image.Image, sparse_depth: np.ndarray,\n",
    "        num_inference_steps: int = 50, processing_resolution: int = 768, seed: int = 2024\n",
    "    ) -> np.ndarray:\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image (PIL.Image.Image): Input image of shape [H, W] with 3 channels.\n",
    "            sparse_depth (np.ndarray): Sparse depth guidance of shape [H, W].\n",
    "            num_inference_steps (int, optional): Number of denoising steps. Defaults to 50.\n",
    "            processing_resolution (int, optional): Resolution for processing. Defaults to 768.\n",
    "            seed (int, optional): Random seed. Defaults to 2024.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Dense depth prediction of shape [H, W].\n",
    "\n",
    "        \"\"\"\n",
    "        # Resolving variables\n",
    "        device = self._execution_device\n",
    "        generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "        # Check inputs.\n",
    "        if num_inference_steps is None:\n",
    "            raise ValueError(\"Invalid num_inference_steps\")\n",
    "        if type(sparse_depth) is not np.ndarray or sparse_depth.ndim != 2:\n",
    "            raise ValueError(\"Sparse depth should be a 2D numpy ndarray with zeros at missing positions\")\n",
    "\n",
    "        # Prepare empty text conditioning\n",
    "        with torch.no_grad():\n",
    "            if self.empty_text_embedding is None:\n",
    "                text_inputs = self.tokenizer(\"\", padding=\"do_not_pad\", \n",
    "                    max_length=self.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "                text_input_ids = text_inputs.input_ids.to(device)\n",
    "                self.empty_text_embedding = self.text_encoder(text_input_ids)[0]  # [1,2,1024]\n",
    "\n",
    "        # Preprocess input images\n",
    "        image, padding, original_resolution = self.image_processor.preprocess(\n",
    "            image, processing_resolution=processing_resolution, device=device, dtype=self.dtype\n",
    "        )  # [N,3,PPH,PPW]\n",
    "\n",
    "        # Check sparse depth dimensions\n",
    "        if sparse_depth.shape != original_resolution:\n",
    "            raise ValueError(\n",
    "                f\"Sparse depth dimensions ({sparse_depth.shape}) must match that of the image ({image.shape[-2:]})\"\n",
    "            )\n",
    "        \n",
    "        # Encode input image into latent space\n",
    "        with torch.no_grad():\n",
    "            image_latent, pred_latent = self.prepare_latents(image, None, generator, 1, 1)  # [N*E,4,h,w], [N*E,4,h,w]\n",
    "        del image\n",
    "\n",
    "        # Preprocess sparse depth\n",
    "        sparse_depth = torch.from_numpy(sparse_depth)[None, None].float().to(device)\n",
    "        sparse_mask = sparse_depth > 0\n",
    "        logging.info(f\"Using {sparse_mask.int().sum().item()} guidance points\")\n",
    "\n",
    "        # Set up optimization targets and compute the range and lower bound of the sparse depth\n",
    "        scale, shift = torch.nn.Parameter(torch.ones(1, device=device)), torch.nn.Parameter(torch.ones(1, device=device))\n",
    "        pred_latent = torch.nn.Parameter(pred_latent)\n",
    "        sparse_range = (sparse_depth[sparse_mask].max() - sparse_depth[sparse_mask].min()).item() # (cmax âˆ’ cmin)\n",
    "        sparse_lower = (sparse_depth[sparse_mask].min()).item() # cmin\n",
    "        \n",
    "        # Set up optimizer\n",
    "        optimizer = torch.optim.Adam([ {\"params\": [scale, shift], \"lr\": 0.005},\n",
    "                                       {\"params\": [pred_latent] , \"lr\": 0.05 }])\n",
    "\n",
    "        def affine_to_metric(depth: torch.Tensor) -> torch.Tensor:\n",
    "            # Convert affine invariant depth predictions to metric depth predictions using the parametrized scale and shift. See Equation 2 of the paper.\n",
    "            return (scale**2) * sparse_range * depth + (shift**2) * sparse_lower\n",
    "\n",
    "        def latent_to_metric(latent: torch.Tensor) -> torch.Tensor:\n",
    "            # Decode latent to affine invariant depth predictions and subsequently to metric depth predictions.\n",
    "            affine_invariant_prediction = self.decode_prediction(latent)  # [E,1,PPH,PPW]\n",
    "            prediction = affine_to_metric(affine_invariant_prediction)\n",
    "            prediction = self.image_processor.unpad_image(prediction, padding)  # [E,1,PH,PW]\n",
    "            prediction = self.image_processor.resize_antialias(\n",
    "                prediction, original_resolution, \"bilinear\", is_aa=False\n",
    "            )  # [1,1,H,W]\n",
    "            return prediction\n",
    "\n",
    "        def loss_l1l2(input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "            # Compute L1 and L2 loss between input and target.\n",
    "            out_l1 = torch.nn.functional.l1_loss(input, target)\n",
    "            out_l2 = torch.nn.functional.mse_loss(input, target)\n",
    "            out = out_l1 + out_l2\n",
    "            return out\n",
    "\n",
    "        # Denoising loop\n",
    "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "        for _, t in enumerate(\n",
    "            self.progress_bar(self.scheduler.timesteps, desc=f\"Marigold-DC steps ({str(device)})...\")\n",
    "        ):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through the U-Net\n",
    "            batch_latent = torch.cat([image_latent, pred_latent], dim=1)  # [1,8,h,w]\n",
    "            noise = self.unet(\n",
    "                batch_latent, t, encoder_hidden_states=self.empty_text_embedding, return_dict=False\n",
    "            )[0]  # [1,4,h,w]\n",
    "\n",
    "            # Compute pred_epsilon to later rescale the depth latent gradient\n",
    "            with torch.no_grad():\n",
    "                alpha_prod_t = self.scheduler.alphas_cumprod[t]\n",
    "                beta_prod_t = 1 - alpha_prod_t\n",
    "                pred_epsilon = (alpha_prod_t**0.5) * noise + (beta_prod_t**0.5) * pred_latent\n",
    "\n",
    "            step_output = self.scheduler.step(noise, t, pred_latent, generator=generator)\n",
    "\n",
    "            # Preview the final output depth with Tweedie's formula (See Equation 1 of the paper)\n",
    "            pred_original_sample = step_output.pred_original_sample\n",
    "\n",
    "            # Decode to metric space, compute loss with guidance and backpropagate\n",
    "            current_metric_estimate = latent_to_metric(pred_original_sample)\n",
    "            loss = loss_l1l2(current_metric_estimate[sparse_mask], sparse_depth[sparse_mask])\n",
    "            loss.backward()\n",
    "\n",
    "            # Scale gradients up\n",
    "            with torch.no_grad():\n",
    "                pred_epsilon_norm = torch.linalg.norm(pred_epsilon).item()\n",
    "                depth_latent_grad_norm = torch.linalg.norm(pred_latent.grad).item()\n",
    "                scaling_factor = pred_epsilon_norm / max(depth_latent_grad_norm, 1e-8)\n",
    "                pred_latent.grad *= scaling_factor\n",
    "\n",
    "            # Execute the update step through guidance backprop\n",
    "            optimizer.step()\n",
    "\n",
    "            # Execute update of the latent with regular denoising diffusion step\n",
    "            with torch.no_grad():\n",
    "                pred_latent.data = self.scheduler.step(noise, t, pred_latent, generator=generator).prev_sample\n",
    "\n",
    "            del pred_original_sample, current_metric_estimate, step_output, pred_epsilon, noise\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        del image_latent\n",
    "\n",
    "        # Decode predictions from latent into pixel space\n",
    "        with torch.no_grad():\n",
    "            prediction = latent_to_metric(pred_latent.detach())\n",
    "\n",
    "        # return Numpy array\n",
    "        prediction = self.image_processor.pt_to_numpy(prediction)  # [N,H,W,1]\n",
    "        self.maybe_free_model_hooks()\n",
    "\n",
    "        return prediction.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH_CHECKPOINT = \"prs-eth/marigold-depth-v1-0\"\n",
    "num_inference_steps = 50\n",
    "processing_resolution = 768\n",
    "\n",
    "# Remove padding from the image\n",
    "image_cropped = ToTensor()(image)\n",
    "image_cropped = image_cropped[:, padding:-padding, :]\n",
    "image_cropped = toPILimage(image_cropped)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    processing_resolution_non_cuda = 512\n",
    "    num_inference_steps_non_cuda = 10\n",
    "    if processing_resolution > processing_resolution_non_cuda:\n",
    "        logging.warning(f\"CUDA not found: Reducing processing_resolution to {processing_resolution_non_cuda}\")\n",
    "        processing_resolution = processing_resolution_non_cuda\n",
    "    if num_inference_steps > num_inference_steps_non_cuda:\n",
    "        logging.warning(f\"CUDA not found: Reducing num_inference_steps to {num_inference_steps_non_cuda}\")\n",
    "        num_inference_steps = num_inference_steps_non_cuda\n",
    "\n",
    "pipe = MarigoldDepthCompletionPipeline.from_pretrained(DEPTH_CHECKPOINT, prediction_type=\"depth\").to(device)\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    logging.warning(\"CUDA not found: Using a lightweight VAE\")\n",
    "    del pipe.vae\n",
    "    pipe.vae = diffusers.AutoencoderTiny.from_pretrained(\"madebyollin/taesd\").to(device)\n",
    "\n",
    "pred = pipe(\n",
    "    image=image_cropped,\n",
    "    sparse_depth=gt_depth_image / 1000.0,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    processing_resolution=processing_resolution,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb1df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "axes[0].imshow(pred, cmap='plasma')\n",
    "axes[0].set_title('Predicted Depth')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(gt_depth_image, cmap='plasma')\n",
    "axes[1].set_title('Ground Truth Depth')\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787384c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.where(gt_depth_image >= 100)  # 100 mm threshold\n",
    "gt_y = gt_depth_image[pos][:, None] / 1000.0  # Convert to meters\n",
    "x = pred[pos][:, None]\n",
    "\n",
    "# Linear regression to find the scale factor\n",
    "scaling_factor = np.linalg.inv(x.T @ x + 0.0) @ (x.T @ gt_y)  # It is scaler so leave the np.eye\n",
    "print(f'Scaling factor: {scaling_factor}')\n",
    "\n",
    "# Apply scaling factor to the predicted depth\n",
    "pred_scaled = pred * scaling_factor[0, 0]\n",
    "\n",
    "# Check MSE\n",
    "se = (x * scaling_factor - gt_y) ** 2\n",
    "mse = np.mean(se)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Max Error: {se.max()}')\n",
    "pos_max = se.argmax()\n",
    "print(f'Max Error Position: {pos_max}, Predicted: {x[pos_max]}, GT: {gt_y[pos_max]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
