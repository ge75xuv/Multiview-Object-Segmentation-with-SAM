{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "\n",
    "project_dir = os.path.dirname(os.getcwd())\n",
    "print(project_dir)\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "from hydra import initialize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import torch\n",
    "from torch.amp import autocast\n",
    "from torchvision.transforms import ToPILImage, Normalize, Resize\n",
    "from torchmetrics.detection import PanopticQuality\n",
    "from training.dataset.transforms import ComposeAPI, NormalizeAPI\n",
    "\n",
    "from helpers.configurations import TRACK_TO_METAINFO, LABEL_PROJECTION_MAP\n",
    "from dataset.collate_fn import collate_fn, collate_fn_wrapper\n",
    "from dataset.mini_dataset import MiniDataset\n",
    "from custom_model_builder import build_sam2former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra.core.global_hydra import GlobalHydra\n",
    "GlobalHydra.instance().is_initialized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size_dict = {\n",
    "    # 'base': {\n",
    "    #     'config': '04_28_00_50/config_resolved.yaml',\n",
    "    #     'ck': '/home/guests/tuna_gurbuz/prototype/sam2_logs/04_28_00_50/checkpoints/checkpoint_33.pt',\n",
    "    #     },\n",
    "    # 'base': {\n",
    "    #     'config': '05_24_10_21/config_resolved.yaml',\n",
    "    #     'ck': '/home/guests/tuna_gurbuz/prototype/sam2_logs/05_24_10_21/checkpoints/checkpoint_34.pt',\n",
    "    #     },\n",
    "    'base1': {\n",
    "        'config': '06_27_17_54/config_resolved.yaml',\n",
    "        'ck': '/home/guests/tuna_gurbuz/prototype/sam2_logs/07_12_10_18/checkpoints/checkpoint_50.pt',\n",
    "        },\n",
    "    'base2': {\n",
    "        'config': '07_12_10_18_def2/config_resolved.yaml',\n",
    "        'ck': '/home/guests/tuna_gurbuz/prototype/sam2_logs/07_12_10_18_def2/checkpoints/checkpoint_51.pt',\n",
    "        },\n",
    "    'base3': {\n",
    "        'config': '07_17_21_33/config_resolved.yaml',\n",
    "        'ck': '/home/guests/tuna_gurbuz/prototype/sam2_logs/07_17_21_33/checkpoints/checkpoint_65.pt',\n",
    "        },\n",
    "    'base4': {\n",
    "        'config': '07_25_11_08/config_resolved.yaml',\n",
    "        'ck': '/home/guests/tuna_gurbuz/prototype/sam2_logs/07_26_09_10/checkpoints/checkpoint_69.pt',\n",
    "        },\n",
    "    'base5': {\n",
    "        'config': '07_26_09_10/config_resolved.yaml',\n",
    "        'ck': '/home/guests/tuna_gurbuz/prototype/sam2_logs/07_26_09_10/checkpoints/checkpoint_75.pt',\n",
    "        },\n",
    "    'base6': {\n",
    "        'config': '07_28_08_49/config_resolved.yaml',\n",
    "        'ck': '/home/guests/tuna_gurbuz/prototype/sam2_logs/07_28_08_49/checkpoints/checkpoint_83.pt',\n",
    "        },\n",
    "    'base7': {\n",
    "        'config': '07_29_16_47/config_resolved.yaml',\n",
    "        'ck': '/home/guests/tuna_gurbuz/prototype/sam2_logs/07_29_16_47/checkpoints/checkpoint_56.pt',\n",
    "        },\n",
    "}\n",
    "# Model\n",
    "model_size = 'base2'\n",
    "config = model_size_dict[model_size]['config']\n",
    "ck = model_size_dict[model_size]['ck']\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "# Hydra init\n",
    "config_path = f'../sam2_logs/{config.split(\"/\")[0]}/'\n",
    "print(\"Config path:\", config_path)\n",
    "config = config.split('/')[1]\n",
    "try:\n",
    "    initialize(version_base=None, config_path=config_path, job_name=\"predict_run\")\n",
    "except ValueError:\n",
    "    pass\n",
    "hydra_overrides = ['scratch.multiview=true', 'trainer.model.multiview=true']\n",
    "amp_type = torch.bfloat16 if device == 'cuda' else torch.float16\n",
    "submodel, object_labels, _, loss, mean, std = build_sam2former(config, ck, device=device, hydra_overrides_extra=hydra_overrides)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submodel.multiview, submodel.epipolar_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(6):\n",
    "    before = submodel.sam_mask_decoder.pixel_decoder.transformer.encoder.layers[ii].self_attn.attention_weights.weight\n",
    "    # print(\"before\", before)\n",
    "    print(torch.all(before == 0))\n",
    "sd = torch.load(ck, map_location=\"cpu\", weights_only=True)[\"model\"]\n",
    "missing_keys, unexpected_keys = submodel.load_state_dict(sd, strict=False)\n",
    "for ii in range(6):\n",
    "    after = submodel.sam_mask_decoder.pixel_decoder.transformer.encoder.layers[ii].self_attn.attention_weights.weight\n",
    "    # print(\"after\", after)\n",
    "    print(torch.all(after == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "len_video = 1\n",
    "input_image_size = 512\n",
    "batch_size = 1\n",
    "shuffle = False\n",
    "# mean = [0.485, 0.456, 0.406]\n",
    "# std = [0.229, 0.224, 0.225]\n",
    "# revert_mean=[-.485/.229, -.456/.224, -.406/.225]\n",
    "# revert_std=[1/.229, 1/.224, 1/.225]\n",
    "#####\n",
    "# mean = [0.3551, 0.3500, 0.3469]\n",
    "# std = [0.2921, 0.2716, 0.2742]\n",
    "# revert_mean=[-.3551/.2921, -.3500/.2716, -.3469/.2742]\n",
    "# revert_std=[1/.2921, 1/.2716, 1/.2742]\n",
    "#####\n",
    "revert_mean = [-mean[0]/std[0], -mean[1]/std[1], -mean[2]/std[2]]\n",
    "revert_std = [1/std[0], 1/std[1], 1/std[2]]\n",
    "transforms = [ComposeAPI([NormalizeAPI(mean=mean, std=std, v2=True)])]\n",
    "revert_transform = Normalize(mean=revert_mean, std=revert_std)\n",
    "test_dataset = MiniDataset('test',\n",
    "                           num_frames=len_video,\n",
    "                           input_image_size=input_image_size,\n",
    "                           object_labels=object_labels,\n",
    "                           transforms=transforms,\n",
    "                           multiview=True,\n",
    "                           collate_fn=collate_fn_wrapper,\n",
    "                           batch_size=batch_size,\n",
    "                           get_seg_mask=True,\n",
    "                           shuffle=shuffle,)\n",
    "print(f'Lenght of the dataset! {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2 # Check seed 123 index 19966\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Image\n",
    "len_objects = len(object_labels)\n",
    "toPILimage = ToPILImage()\n",
    "exist = False\n",
    "if_break = False\n",
    "# sample_idx = 950\n",
    "# sample_idx = 2000\n",
    "# sample_idx = 1250\n",
    "# # sample_idx = 3993\n",
    "# sample_idx = 278\n",
    "sample_idx = 500\n",
    "test_loader = test_dataset.get_loader()\n",
    "flag_iter = False\n",
    "\n",
    "# Run the model\n",
    "with torch.no_grad():\n",
    "    submodel.eval()\n",
    "    if flag_iter:\n",
    "        batch = next(itertools.islice(test_loader, sample_idx, None))\n",
    "    else:\n",
    "        batch = [test_dataset[i] for i in range(sample_idx * batch_size, (sample_idx+1) * batch_size) if i < len(test_dataset)]\n",
    "        batch = collate_fn_wrapper(batch, 'val')\n",
    "    batched_video_data_val = batch.to(device)\n",
    "    # batch_seg_mask_gt = batch[1]  # List of PIL Image for debug\n",
    "    masks_val0 = batch[0].masks.to(device)\n",
    "    masks_val1 = batch[1].masks.to(device)\n",
    "    masks_val2 = batch[2].masks.to(device)\n",
    "    with autocast(device_type=device, dtype=amp_type):\n",
    "        all_frame_outputs_val = submodel(batched_video_data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[test_dataset.images[i] for i in range(sample_idx * batch_size, (sample_idx+1) * batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frame_outputs_val.keys(), all_frame_outputs_val[0].keys(), all_frame_outputs_val[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_outputs(view_output, unique_object_identfier):\n",
    "    # Convert mask prediction logits to binary masks\n",
    "    binary_pred_masks = view_output[0]['pred_masks_high_res'].sigmoid() > 0.5\n",
    "    binary_pred_masks = binary_pred_masks.cpu().numpy()\n",
    "\n",
    "    # Convert class logits to class predictions\n",
    "    pred_logits = view_output[0]['pred_logits'].type(torch.float32).softmax(-1).cpu().numpy()\n",
    "    pred_class = pred_logits.argmax(-1)\n",
    "\n",
    "    gt_labels = unique_object_identfier[:,:,1]\n",
    "    return binary_pred_masks, pred_class, gt_labels\n",
    "\n",
    "binary_pred_masks0, pred_class0, gt_labels0 = process_outputs(all_frame_outputs_val[0], batched_video_data_val[0].metadata.unique_objects_identifier)\n",
    "binary_pred_masks1, pred_class1, gt_labels1 = process_outputs(all_frame_outputs_val[1], batched_video_data_val[1].metadata.unique_objects_identifier)\n",
    "binary_pred_masks2, pred_class2, gt_labels2 = process_outputs(all_frame_outputs_val[2], batched_video_data_val[2].metadata.unique_objects_identifier)\n",
    "\n",
    "B, N, H, W = binary_pred_masks0.shape  # Size with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# dump_dict = {k: v.cpu().type(torch.float32).numpy().tolist() for k,v in all_frame_outputs_val[0].items() if k != 'aux_outputs'}\n",
    "# with open('../temp/predictions.json', 'w') as f:\n",
    "#     json.dump(dump_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class0, gt_labels0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS PART IS ONLY VALID IF ALL THE IMAGES ARE AZURE\n",
    "# # Ground truth mask\n",
    "# seg_mask_gt = np.array(batch_seg_mask_gt)  # Convert from PIL to numpy\n",
    "# seg_mask_gt = seg_mask_gt.squeeze(1) if len(seg_mask_gt.shape) == 5 else seg_mask_gt  # Reshape to 3 dimensions\n",
    "\n",
    "# # Sizes and resizing\n",
    "# B, N, H, W = binary_pred_masks.shape  # Size with padding\n",
    "# H_full, W_full = seg_mask_gt.shape[-3:-1]\n",
    "# resize_factor = W_full // W\n",
    "# resize_shape = [H_full // resize_factor, W_full // resize_factor]\n",
    "# resize_image = Resize(resize_shape)\n",
    "\n",
    "# # Revert the padded mask coming out of the model\n",
    "# padding_size = (H - resize_shape[0]) // 2\n",
    "# binary_pred_masks = binary_pred_masks[:,:, padding_size : H-padding_size, :]\n",
    "# H, W = binary_pred_masks.shape[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_map():\n",
    "    color_map = {}\n",
    "    for k, v in LABEL_PROJECTION_MAP['default'].items():\n",
    "        # if k > 7 and v['label'] == 6:\n",
    "        #     continue\n",
    "        r, g, b = v['color']\n",
    "        color_map[v['label']] = (r,g,b)\n",
    "    return color_map\n",
    "\n",
    "# Get color map\n",
    "color_map = get_color_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_seg_rgb_masks(binary_pred_masks, pred_class, color_map, manip=False):\n",
    "    \"\"\"\n",
    "    Prepare segmentation mask and RGB mask with color encoding.\n",
    "    \"\"\"\n",
    "    B, H, W = binary_pred_masks.shape[0], binary_pred_masks.shape[2], binary_pred_masks.shape[3]\n",
    "    pred_seg_class_mask = np.ones((B, 1, H, W), dtype=np.int8) * 23  # 16 is the background class\n",
    "    pred_rgb_mask = np.zeros((B, H, W, 3), dtype=np.uint8)\n",
    "    \n",
    "    for _b_idx, (_b_binary_pred_masks, _b_pred_class) in enumerate(zip(binary_pred_masks, pred_class)):\n",
    "        for mask, class_id in zip(_b_binary_pred_masks, _b_pred_class):\n",
    "            pos = np.where(mask == True)\n",
    "            if len(pos[0]) > 0:\n",
    "                pred_seg_class_mask[_b_idx, 0, pos[0], pos[1]] = class_id\n",
    "                if manip:\n",
    "                    color = color_map[class_id] if class_id != 10 else color_map[17]\n",
    "                else:\n",
    "                    color = color_map[class_id]\n",
    "                pred_rgb_mask[_b_idx, pos[0], pos[1], :] = color\n",
    "            else:\n",
    "                continue\n",
    "    return pred_seg_class_mask, pred_rgb_mask\n",
    "\n",
    "pred_seg_class_mask0, pred_rgb_mask0 = _prepare_seg_rgb_masks(binary_pred_masks0, pred_class0, color_map, True)\n",
    "pred_seg_class_mask1, pred_rgb_mask1 = _prepare_seg_rgb_masks(binary_pred_masks1, pred_class1, color_map)\n",
    "pred_seg_class_mask2, pred_rgb_mask2 = _prepare_seg_rgb_masks(binary_pred_masks2, pred_class2, color_map)\n",
    "\n",
    "# # Prepare segmentation mask and RGB mask with color encoding\n",
    "# pred_seg_class_mask = np.ones((B, 1, H, W), dtype=np.int8) * 23  # 16 is the background class\n",
    "# pred_rgb_mask = np.zeros((B, H, W, 3), dtype=np.uint8)\n",
    "# for _b_idx, (_b_binary_pred_masks, _b_pred_class) in enumerate(zip(binary_pred_masks, pred_class)):\n",
    "#     #TODO if class_id == background pass\n",
    "#     for mask, class_id in zip(_b_binary_pred_masks, _b_pred_class):\n",
    "#         pos = np.where(mask == True)\n",
    "#         if len(pos[0]) > 0:\n",
    "#             pred_seg_class_mask[_b_idx, 0, pos[0], pos[1]] = class_id\n",
    "#             pred_rgb_mask[_b_idx, pos[0], pos[1], :] = color_map[class_id]\n",
    "#         else:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_gt_seg_rgb_masks(masks_val, gt_labels, obj_to_frame_idx, color_map):\n",
    "    \"\"\"\n",
    "    Prepare ground truth segmentation mask and RGB mask with color encoding.\n",
    "    \"\"\"\n",
    "    B, H, W = masks_val.shape[0], masks_val.shape[2], masks_val.shape[3]\n",
    "    # Initialize masks\n",
    "    gt_seg_class_mask = np.ones((B, 1, H, W), dtype=np.int8) * 23  # 16 is the background class\n",
    "    gt_rgb_mask = np.zeros((B, H, W, 3), dtype=np.uint8)\n",
    "    for batch_idx in range(B):\n",
    "        pos = obj_to_frame_idx[0,:,1] == batch_idx  # The objects in the batch are mixed\n",
    "        gt_mask = masks_val[0, pos, :, :].cpu().numpy()  # Get the GT masks in the batch\n",
    "        gt_class_id = gt_labels[0, pos].cpu().numpy()  # Get the GT class id in the batch\n",
    "        for mask, class_id in zip(gt_mask, gt_class_id):\n",
    "            pos = np.where(mask == True)\n",
    "            if len(pos[0]) > 0:\n",
    "                gt_seg_class_mask[batch_idx, 0, pos[0], pos[1]] = class_id\n",
    "                gt_rgb_mask[batch_idx, pos[0], pos[1], :] = color_map[class_id]\n",
    "            else:\n",
    "                continue\n",
    "    return gt_seg_class_mask, gt_rgb_mask\n",
    "\n",
    "gt_seg_class_mask0, gt_rgb_mask0 = _prepare_gt_seg_rgb_masks(masks_val0, gt_labels0, batched_video_data_val[0].obj_to_frame_idx, color_map)\n",
    "gt_seg_class_mask1, gt_rgb_mask1 = _prepare_gt_seg_rgb_masks(masks_val1, gt_labels1, batched_video_data_val[1].obj_to_frame_idx, color_map)\n",
    "gt_seg_class_mask2, gt_rgb_mask2 = _prepare_gt_seg_rgb_masks(masks_val2, gt_labels2, batched_video_data_val[2].obj_to_frame_idx, color_map)\n",
    "\n",
    "# gt_seg_class_mask = np.ones((B, 1, H, W), dtype=np.int8) * 23  # 16 is the background class\n",
    "# gt_rgb_mask = np.zeros((B, H, W, 3), dtype=np.uint8)\n",
    "# for batch_idx in range(B):\n",
    "#     pos = batched_video_data_val.obj_to_frame_idx[0,:,1] == batch_idx  # The objects in the batch are mixed\n",
    "#     gt_mask = masks_val[0, pos, :, :].cpu().numpy()  # Get the GT masks in the batch\n",
    "#     gt_class_id = gt_labels[0, pos].cpu().numpy()  # Get the GT class id in the batch\n",
    "#     for mask, class_id in zip(gt_mask, gt_class_id):\n",
    "#         pos = np.where(mask == True)\n",
    "#         if len(pos[0]) > 0:\n",
    "#             gt_seg_class_mask[batch_idx, 0, pos[0], pos[1]] = class_id\n",
    "#             gt_rgb_mask[batch_idx, pos[0], pos[1], :] = color_map[class_id]\n",
    "#         else:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS PART IS ONLY VALID IF ALL THE IMAGES ARE AZURE\n",
    "# Prepare GT segmentation mask and RGB mask with color encoding\n",
    "# gt_seg_class_mask = np.ones((B, 1, H, W), dtype=np.int8) * 16  # 16 is the background class\n",
    "# gt_rgb_mask = np.zeros((B, H_full, W_full, 3), dtype=np.uint8)\n",
    "# for _b_idx, _b_seg_mask_gt in enumerate(seg_mask_gt):\n",
    "#     for label in object_labels:\n",
    "#         downsample_gt_mask = resize_image(torch.tensor(_b_seg_mask_gt[:,:,1] == label).unsqueeze(0))[0]\n",
    "#         pos_dws = np.where(downsample_gt_mask == True)\n",
    "#         pos_rgb = np.where(_b_seg_mask_gt[:,:,1] == label)\n",
    "#         if len(pos_rgb[0]) > 0:\n",
    "#             projected_label = LABEL_PROJECTION_MAP[label]['label']\n",
    "#             gt_rgb_mask[_b_idx, pos_rgb[0], pos_rgb[1], :] = color_map[projected_label]\n",
    "#             gt_seg_class_mask[_b_idx, 0, pos_dws[0], pos_dws[1]] = projected_label\n",
    "#         else:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for masks\n",
    "# pred_rgb_mask_list_azure = []\n",
    "# pred_seg_class_mask_list_azure = []\n",
    "# gt_rgb_mask_list_azure = []\n",
    "# gt_seg_class_mask_list_azure = []\n",
    "\n",
    "# pred_rgb_mask_list_sim = []  # Auxillary lists for now\n",
    "# pred_seg_class_mask_list_sim = []  # Auxillary lists for now\n",
    "# gt_rgb_mask_list_sim = []  # Auxillary lists for now\n",
    "# gt_seg_class_mask_list_sim = []  # Auxillary lists for now\n",
    "\n",
    "# # Revert the padding\n",
    "# for batch_idx in range(B):\n",
    "#     # Load the corresponding rgb image to look for padding \n",
    "#     img = batched_video_data_val.img_batch[0][batch_idx].permute(1,2,0).cpu().numpy()\n",
    "    \n",
    "#     # Check if the image is black (0,0,0) along the height dimension\n",
    "#     is_black_row = (img[:,:,0] <= 0)  # Check along width and channels\n",
    "#     pos_padding = np.where(is_black_row == False)\n",
    "#     start = pos_padding[0][0]\n",
    "#     end = pos_padding[0][-1]\n",
    "    \n",
    "#     # Slice the predictions and ground truth masks\n",
    "#     if start == 64:\n",
    "#         pred_rgb_mask_list_azure.append(pred_rgb_mask[batch_idx, start:end+1, :, :])\n",
    "#         pred_seg_class_mask_list_azure.append(pred_seg_class_mask[batch_idx, 0:1, start:end+1, :])\n",
    "#         gt_rgb_mask_list_azure.append(gt_rgb_mask[batch_idx, start:end+1, :, :])\n",
    "#         gt_seg_class_mask_list_azure.append(gt_seg_class_mask[batch_idx, 0:1, start:end+1, :])\n",
    "#     elif start == 0:\n",
    "#         pass\n",
    "#     else:\n",
    "#         raise ValueError(\"Padding not found in the image\")\n",
    "\n",
    "# # Concatenate the masks\n",
    "# pred_rgb_mask = np.array(pred_rgb_mask_list_azure)\n",
    "# pred_seg_class_mask = np.array(pred_seg_class_mask_list_azure)\n",
    "# gt_rgb_mask = np.array(gt_rgb_mask_list_azure)\n",
    "# gt_seg_class_mask = np.array(gt_seg_class_mask_list_azure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color encoded segmentation masks and label encoded segmentation masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rgb_mask0.shape, pred_seg_class_mask0.shape, gt_rgb_mask0.shape, gt_seg_class_mask0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_masks(pred_rgb_mask, gt_rgb_mask, pred_seg_class_mask, gt_seg_class_mask):\n",
    "    \"\"\"\n",
    "    Visualize the predicted and ground truth masks.\n",
    "    \"\"\"\n",
    "    B = pred_rgb_mask.shape[0]\n",
    "    figsize = np.array([4, B]) * 3\n",
    "    fig, axs = plt.subplots(B, 4, figsize=figsize)\n",
    "\n",
    "    if B == 1:\n",
    "        axs = np.expand_dims(axs, axis=0)\n",
    "    for ii in range(B):\n",
    "        axs[ii,0].imshow(pred_rgb_mask[ii])\n",
    "        axs[ii,0].axis('off')\n",
    "        axs[ii,0].set_title('PRED')\n",
    "\n",
    "        axs[ii,1].imshow(gt_rgb_mask[ii])\n",
    "        axs[ii,1].axis('off')\n",
    "        axs[ii,1].set_title('GT')\n",
    "\n",
    "        axs[ii,2].imshow(pred_seg_class_mask[ii, 0])\n",
    "        axs[ii,2].axis('off')\n",
    "        axs[ii,2].set_title('PRED')\n",
    "\n",
    "        axs[ii,3].imshow(gt_seg_class_mask[ii, 0])\n",
    "        axs[ii,3].axis('off')\n",
    "        axs[ii,3].set_title('GT')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_masks(pred_rgb_mask0, gt_rgb_mask0, pred_seg_class_mask0, gt_seg_class_mask0)\n",
    "visualize_masks(pred_rgb_mask1, gt_rgb_mask1, pred_seg_class_mask1, gt_seg_class_mask1)\n",
    "visualize_masks(pred_rgb_mask2, gt_rgb_mask2, pred_seg_class_mask2, gt_seg_class_mask2)\n",
    "\n",
    "# figsize = np.array([4, B]) * 3\n",
    "# fig, axs = plt.subplots(B, 4, figsize=figsize, )\n",
    "\n",
    "# for ii in range(B):\n",
    "#     axs[ii,0].imshow(pred_rgb_mask[ii])\n",
    "#     axs[ii,0].axis('off')\n",
    "#     axs[ii,0].set_title('PRED')\n",
    "\n",
    "#     axs[ii,1].imshow(gt_rgb_mask[ii])\n",
    "#     axs[ii,1].axis('off')\n",
    "#     axs[ii,1].set_title('GT')\n",
    "\n",
    "#     axs[ii,2].imshow(pred_seg_class_mask[ii, 0])\n",
    "#     axs[ii,2].axis('off')\n",
    "#     axs[ii,2].set_title('PRED')\n",
    "\n",
    "#     axs[ii,3].imshow(gt_seg_class_mask[ii, 0])\n",
    "#     axs[ii,3].axis('off')\n",
    "#     axs[ii,3].set_title('GT')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Panoptic Quality (PQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuff: 16, Things: 17 Instance labels: 0-15\n",
    "background = 23\n",
    "things = {LABEL_PROJECTION_MAP['default'][idx]['label'] for idx in object_labels}\n",
    "stuff = {background}  # Putting 17 does not change anything for the last value\n",
    "print(f'Things: {things}\\nStuff: {stuff}')\n",
    "\n",
    "def calculate_panoptic_quality(pred_seg_class_mask, gt_seg_class_mask):\n",
    "    \"\"\"\n",
    "    Calculate panoptic quality metric.\n",
    "    \"\"\"\n",
    "    # Prepare stuff and things mask for prediction\n",
    "    preds = pred_seg_class_mask[:, 0, :, :, None]\n",
    "    instance_ids = np.zeros_like(preds)  # Almost like semantic segmentation, therefore all objects are the first instances\n",
    "    preds = np.concatenate((preds, instance_ids), axis=3)\n",
    "\n",
    "    # Prepare stuff and things mask for GT\n",
    "    target = gt_seg_class_mask[:, 0, :, :, None]\n",
    "    instance_ids_gt = np.zeros_like(target)  # Almost like semantic segmentation, therefore all objects are the first instances\n",
    "    target = np.concatenate((target, instance_ids_gt), axis=3)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    preds = torch.tensor(preds)\n",
    "    target = torch.tensor(target)\n",
    "\n",
    "    # Prepare panoptic quality metric\n",
    "    panoptic_quality = PanopticQuality(stuffs=stuff, things=things)\n",
    "    pq = panoptic_quality(preds, target)\n",
    "    print(f'Prediction: {preds.shape}, Target: {target.shape}')\n",
    "    print(f\"PQ: {pq}\")\n",
    "    return pq\n",
    "\n",
    "# Calculate PQ\n",
    "pq0 = calculate_panoptic_quality(pred_seg_class_mask0, gt_seg_class_mask0)\n",
    "pq1 = calculate_panoptic_quality(pred_seg_class_mask1, gt_seg_class_mask1)\n",
    "pq2 = calculate_panoptic_quality(pred_seg_class_mask2, gt_seg_class_mask2)\n",
    "\n",
    "average_pq = (pq0 + pq1 + pq2) / 3\n",
    "print(f\"Average PQ: {average_pq}\")\n",
    "\n",
    "if False:\n",
    "    # Corrected PQ calculation\n",
    "    background = 23\n",
    "    things = {LABEL_PROJECTION_MAP[idx]['label'] for idx in object_labels}\n",
    "    stuff = {background}  # Only background is stuff\n",
    "    print(f'Things: {things}\\nStuff: {stuff}')\n",
    "\n",
    "    # Create panoptic masks with unique instance IDs\n",
    "    pred_panoptic = np.full((B, H, W), background, dtype=np.int32)  # Start with background\n",
    "    target_panoptic = np.full((B, H, W), background, dtype=np.int32)\n",
    "\n",
    "    # For predictions\n",
    "    for batch_idx in range(B):\n",
    "        for mask_idx, (mask, class_id) in enumerate(zip(binary_pred_masks[batch_idx], pred_class[batch_idx])):\n",
    "            if mask.any():  # If mask is not empty\n",
    "                pred_panoptic[batch_idx][mask] = class_id\n",
    "    instance_ids = np.zeros_like(pred_panoptic)  # Almost like semantic segmentation, therefore all objects are the first instances\n",
    "    pred_panoptic = np.stack((pred_panoptic, instance_ids), axis=3)\n",
    "\n",
    "    # For ground truth\n",
    "    for batch_idx in range(B):\n",
    "        pos = batched_video_data_val.obj_to_frame_idx[0,:,1] == batch_idx\n",
    "        gt_mask = masks_val[0, pos, :, :].cpu().numpy()\n",
    "        for mask_idx, mask in enumerate(gt_mask):\n",
    "            if mask.any():  # If mask is not empty\n",
    "                target_panoptic[batch_idx][mask] = mask_idx\n",
    "    instance_ids_gt = np.zeros_like(target_panoptic)  # Almost like semantic segmentation, therefore all objects are the first instances\n",
    "    target_panoptic = np.stack((target_panoptic, instance_ids_gt), axis=3)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    pred_panoptic = torch.tensor(pred_panoptic)\n",
    "    target_panoptic = torch.tensor(target_panoptic)\n",
    "\n",
    "    # Calculate PQ\n",
    "    panoptic_quality = PanopticQuality(things=things, stuffs=stuff)\n",
    "    pq = panoptic_quality(pred_panoptic, target_panoptic)\n",
    "    print(f\"Corrected PQ: {pq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = np.array([3, 2]) * 5\n",
    "fig, axs = plt.subplots(2, 3, figsize=fig_size)\n",
    "for ii in range(3):\n",
    "    image = batched_video_data_val[ii][0].img_batch[0]\n",
    "    unnormalized_image = toPILimage(revert_transform(image))\n",
    "    axs[0, ii].imshow(unnormalized_image)\n",
    "    axs[0, ii].axis('off')\n",
    "    # axs[0, ii].set_title(f\"Image {ii_to_cam_dict[ii]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Masks on Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = batch[0]\n",
    "# batch_size = len(batch.img_batch)\n",
    "# key = batch.dict_key  # key for dataset\n",
    "# targets = []\n",
    "\n",
    "# for i in range(batch_size):\n",
    "#     # dim=2 video_id, obj_id, frame_id\n",
    "#     xx, yy = torch.where(batch.metadata.unique_objects_identifier[:,:,2] == i)\n",
    "#     obj_id = batch.metadata.unique_objects_identifier[xx,yy,1]\n",
    "#     targets.append({\n",
    "#         \"masks\": batch.masks[i],\n",
    "#         \"labels\": obj_id,\n",
    "#     })\n",
    "    \n",
    "# loss[key](all_frame_outputs_val, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batched_video_data_val.img_batch.shape = (N, B, C, H, W)  \n",
    "masks_val.shape = (N, B*O, H, W)  \n",
    "metadata.unique_objects_identifier = (N, B*O, 3) => [video_id, object_id, frame_id]  \n",
    "obj_to_frame_idx = (N, B*O, 2) => [video_id, frame_id]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_map():\n",
    "    color_map = []\n",
    "    for k, v in LABEL_PROJECTION_MAP['default'].items():\n",
    "        # if k > 7 and v['label'] == 6:\n",
    "        #     continue\n",
    "        r, g, b = v['color']\n",
    "        color_map.append(sv.Color(r,g,b))\n",
    "    color_map = sv.ColorPalette(colors=color_map)\n",
    "    return color_map\n",
    "\n",
    "# Get color map\n",
    "color_map = get_color_map()\n",
    "\n",
    "# GT Labels\n",
    "gt_labels = batched_video_data_val.metadata.unique_objects_identifier[:,:,1]\n",
    "\n",
    "# Create subplots\n",
    "figsize = np.array([2, B]) * 4\n",
    "fig, axs = plt.subplots(B, 2, figsize=figsize, )\n",
    "\n",
    "for batch_idx in range(B):\n",
    "    # Unnormalize the image\n",
    "    image = batched_video_data_val.img_batch[0][batch_idx]\n",
    "    unnormalized_image = toPILimage(revert_transform(image))\n",
    "\n",
    "    # GT masks\n",
    "    pos = batched_video_data_val.obj_to_frame_idx[0,:,1] == batch_idx\n",
    "    gt_mask = masks_val[0, pos, :, :].cpu().numpy()\n",
    "    gt_class_id = gt_labels[0, pos].cpu().numpy()\n",
    "    empty_bboxes = np.array([[0, 0, 0, 0]] * len(gt_class_id))\n",
    "\n",
    "    # Gt Annotated Frame \n",
    "    gt_detections = sv.Detections(xyxy=empty_bboxes, mask=gt_mask, class_id=gt_class_id)\n",
    "    mask_annotator = sv.MaskAnnotator(color=color_map)\n",
    "    gt_annotated_frame = mask_annotator.annotate(\n",
    "        scene=unnormalized_image.copy(),\n",
    "        detections=gt_detections\n",
    "    )\n",
    "    # sv.plot_image(gt_annotated_frame, size=(4,4)) if B <= 10 else None\n",
    "\n",
    "    # Prediction masks\n",
    "    pred_mask = (all_frame_outputs_val[0]['pred_masks_high_res'][batch_idx].sigmoid() > 0.5).cpu().numpy()\n",
    "    # pred_class_ids = best_probs.indices.numpy().astype(np.int32)\n",
    "    empty_bboxes = np.array([[0, 0, 0, 0]] * len(pred_class[0]))\n",
    "\n",
    "    # Prediction Annotated Frame\n",
    "    pred_detections = sv.Detections(xyxy=empty_bboxes, mask=pred_mask, class_id=pred_class[batch_idx])\n",
    "    mask_annotator = sv.MaskAnnotator(color=color_map)\n",
    "    pred_annotated_frame = mask_annotator.annotate(\n",
    "        scene=unnormalized_image.copy(),\n",
    "        detections=pred_detections\n",
    "    )\n",
    "    # sv.plot_image(pred_annotated_frame, size=(4,4)) if B <= 10 else None\n",
    "    axs[batch_idx,0].imshow(pred_annotated_frame, )\n",
    "    axs[batch_idx,0].axis('off')\n",
    "    axs[batch_idx,0].set_title(f'PRED w PQ: {pq:.4f}')\n",
    "\n",
    "    axs[batch_idx,1].imshow(gt_annotated_frame)\n",
    "    axs[batch_idx,1].axis('off')\n",
    "    axs[batch_idx,1].set_title('GT')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig('../temp/pred_gt.png', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii_to_cam_dict = {0: 'Cam1', 1: 'Cam4', 2: 'Cam5'}\n",
    "fig_size = np.array([3, 2]) * 5\n",
    "fig, axs = plt.subplots(2, 3, figsize=fig_size)\n",
    "for ii in range(3):\n",
    "    image = batched_video_data_val.img_batch[0][ii]\n",
    "    unnormalized_image = toPILimage(revert_transform(image))\n",
    "    axs[0, ii].imshow(unnormalized_image)\n",
    "    axs[0, ii].axis('off')\n",
    "    axs[0, ii].set_title(f\"Image {ii_to_cam_dict[ii]}\")\n",
    "\n",
    "    axs[1, ii].imshow(gt_rgb_mask[ii])\n",
    "    axs[1, ii].axis('off')\n",
    "    axs[1, ii].set_title(f\"GT {ii_to_cam_dict[ii]}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
