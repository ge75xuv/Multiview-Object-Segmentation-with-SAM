{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "\n",
    "project_dir = os.path.dirname(os.getcwd())\n",
    "print(project_dir)\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import ToPILImage, ToTensor, Normalize, Resize\n",
    "from training.dataset.transforms import ComposeAPI, NormalizeAPI\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset.collate_fn import collate_fn\n",
    "from dataset.mini_dataset import MiniDataset\n",
    "from helpers.configurations import LABEL_PROJECTION_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "object_labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 24, 25, 26, 27, 28, 29]\n",
    "len_video = 1\n",
    "input_image_size = 512\n",
    "batch_size = 1\n",
    "shuffle = False\n",
    "multiview = True\n",
    "mean = [0.3551, 0.3500, 0.3469]\n",
    "std = [0.2921, 0.2716, 0.2742]\n",
    "transforms = [ComposeAPI([NormalizeAPI(mean=mean, std=std, v2=True)])]\n",
    "revert_mean=[-.3551/.2921, -.3500/.2716, -.3469/.2742]\n",
    "revert_std=[1/.2921, 1/.2716, 1/.2742]\n",
    "revert_transform = Normalize(mean=revert_mean, std=revert_std)\n",
    "test_dataset = MiniDataset('train',\n",
    "                           num_frames=len_video,\n",
    "                           input_image_size=input_image_size,\n",
    "                           object_labels=object_labels,\n",
    "                           transforms=transforms,\n",
    "                           collate_fn=collate_fn,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=shuffle,\n",
    "                           multiview=multiview,)\n",
    "print(f'Lenght of the dataset! {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3 * 1000\n",
    "test_dataset.images[idx], test_dataset.images[idx+1], test_dataset.images[idx+2], len(test_dataset.images), len(test_dataset)\n",
    "# test_dataset.segmentation_masks[idx], test_dataset.segmentation_masks[idx+1], test_dataset.segmentation_masks[idx+2], len(test_dataset.segmentation_masks), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(test_dataset.images), 3):\n",
    "    # print(f'Image {i}: {test_dataset.images[i]}')\n",
    "    # print(f'Image {i+1}: {test_dataset.images[i+1]}')\n",
    "    # print(f'Image {i+2}: {test_dataset.images[i+2]}')\n",
    "    # print('---')\n",
    "    name1 = str(test_dataset.images[i][0]).split('/')[-1].split('_')[0]\n",
    "    timestamp1 = str(test_dataset.images[i][0]).split('/')[-1].split('_')[1]\n",
    "    assert name1 == 'camera01'\n",
    "    name2 = str(test_dataset.images[i+1][0]).split('/')[-1].split('_')[0]\n",
    "    timestamp2 = str(test_dataset.images[i+1][0]).split('/')[-1].split('_')[1]\n",
    "    assert name2 == 'camera04'\n",
    "    name3 = str(test_dataset.images[i+2][0]).split('/')[-1].split('_')[0]\n",
    "    timestamp3 = str(test_dataset.images[i+2][0]).split('/')[-1].split('_')[1]\n",
    "    assert name3 == 'camera05'\n",
    "    assert timestamp1 == timestamp2 == timestamp3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiview Point Cloud Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_int_ext = [(mtrx[0].numpy(), mtrx[1].numpy()) for mtrx in test_dataset.camera_features['001_PKA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_int = [mtrx[0] for mtrx in cam_int_ext]\n",
    "cam_ext = [mtrx[1] for mtrx in cam_int_ext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(mv_idx, img_indices):\n",
    "    # Get image path\n",
    "    img_paths = [test_dataset.images[i][0] for i in img_indices]\n",
    "    # img_paths = test_dataset.images[img_indices, :].squeeze(1)  # get image paths for the multiview indices\n",
    "    # Load video data batch\n",
    "    video_data_batch = test_dataset[mv_idx]\n",
    "    # Inverse mean std normalization\n",
    "    img0 = revert_transform(video_data_batch[0][0].frames[0].data).numpy()\n",
    "    img1 = revert_transform(video_data_batch[0][1].frames[0].data).numpy()\n",
    "    img2 = revert_transform(video_data_batch[0][2].frames[0].data).numpy()\n",
    "    img = (img0, img1, img2)\n",
    "    # Depth path\n",
    "    depth_images = []\n",
    "    for img_path in img_paths:\n",
    "        last_part = str(img_path.parts[-1].split('/')[-1].replace('.jpg', '.tiff').replace('color', 'depth'))\n",
    "        depth_path = img_path.parents[1] / 'depthimage' / last_part\n",
    "        # Load depth image\n",
    "        gt_depth_image = Image.open(depth_path)\n",
    "        # Resize depth image to 512x512\n",
    "        gt_depth_image = gt_depth_image.resize((gt_depth_image.size[0]//2, gt_depth_image.size[1]//2))\n",
    "        depth_images.append(np.array(gt_depth_image))\n",
    "    return img, depth_images, depth_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dataset = len(test_dataset)\n",
    "print(f'Length of the dataset: {len_dataset}')\n",
    "mv_indices = [1115]  # [700, 1115, 1442]\n",
    "img_views = []\n",
    "depth_views = []\n",
    "\n",
    "for mv_idx in tqdm(mv_indices):\n",
    "    # Get image path and camera index\n",
    "    img_indices = [mv_idx * 3 + kk for kk in range(3)]\n",
    "    img, gt_depth_image, depth_path = process_img(mv_idx, img_indices)\n",
    "    img = np.stack(img, axis=0)  # Stack images along a new dimension\n",
    "    gt_depth_image = np.stack(gt_depth_image, axis=0)  # Stack depth images along a new dimension\n",
    "    H,W = gt_depth_image.shape[-2:]\n",
    "    buffer = (W-H) // 2\n",
    "    img = img[:,:, buffer:-buffer, :]\n",
    "    assert img.shape[-2:] == gt_depth_image.shape[-2:]\n",
    "    img_views.append(img)\n",
    "    depth_views.append(gt_depth_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_path, test_dataset.images[img_indices[0]], test_dataset.images[img_indices[1]], test_dataset.images[img_indices[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_seg_masks = False\n",
    "tool_name = {1115: 'drill', 1442: 'hammer', 700: 'saw'}\n",
    "if download_seg_masks:\n",
    "    for idx in range(3):\n",
    "        seg_mask = Image.open(test_dataset.segmentation_masks[img_indices[idx]][0]).convert(\"L\").resize((W,H), resample=Image.NEAREST)\n",
    "        seg_mask_np = np.array(seg_mask)\n",
    "        color_mask = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "        for i in LABEL_PROJECTION_MAP['default'].keys():\n",
    "            if i == 0:\n",
    "                continue\n",
    "            mask = seg_mask_np==i\n",
    "            color_mask[mask, :] = LABEL_PROJECTION_MAP['default'][i]['color']\n",
    "        color_mask_pil = Image.fromarray(color_mask).save(f'../temp/img_view_{idx}_{tool_name[mv_idx]}_segmask.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_pil = ToPILImage()\n",
    "\n",
    "# img_hammer_1 = convert_pil(torch.tensor(img_views[0][1])).save(f'../temp/img_view_1_{tool_name[mv_idx]}.png')\n",
    "# img_hammer_0 = convert_pil(torch.tensor(img_views[0][0])).save(f'../temp/img_view_0_{tool_name[mv_idx]}.png')\n",
    "# img_hammer_2 = convert_pil(torch.tensor(img_views[0][2])).save(f'../temp/img_view_2_{tool_name[mv_idx]}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case if we need other camera views\n",
    "all_cameras = False\n",
    "if all_cameras:\n",
    "    # Get the path\n",
    "    camera01_path = test_dataset.images[img_indices[0]][0]\n",
    "    parent = camera01_path.parents[0]\n",
    "    orig = camera01_path.parts[-1]\n",
    "    camera02_name = orig[:7] + \"2\" + orig[8:]\n",
    "    camera03_name = orig[:7] + \"3\" + orig[8:]\n",
    "    camera02_path = parent / camera02_name\n",
    "    camera03_path = parent / camera03_name\n",
    "    \n",
    "    # Open the images, reshape and save\n",
    "    test1 = Image.open(camera02_path).resize((W, H)).save(f'../temp/img_view_3_{tool_name[mv_idx]}.png')\n",
    "    test2 = Image.open(camera03_path).resize((W, H)).save(f'../temp/img_view_4_{tool_name[mv_idx]}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(15, 7))\n",
    "for i, im in enumerate(img):\n",
    "    axs[0, i].imshow(im.transpose(1, 2, 0))  # Change to HxWxC for plotting\n",
    "    axs[0, i].set_title(f'Image {i+1}')\n",
    "    axs[0, i].axis('off')\n",
    "    \n",
    "    axs[1, i].imshow(gt_depth_image[i], cmap='plasma')  # Change to HxW for plotting\n",
    "    axs[1, i].set_title(f'Depth {i+1}')\n",
    "    axs[1, i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_flag_dump = True\n",
    "if _flag_dump:\n",
    "    with open(f'../temp/mv_img_views{mv_idx}.npy', 'wb') as f:\n",
    "        np.save(f, np.array(img_views))\n",
    "    with open(f'../temp/mv_depth_views{mv_idx}.npy', 'wb') as f:\n",
    "        np.save(f, np.array(depth_views))\n",
    "    with open(f'../temp/camera_int{mv_idx}.npy', 'wb') as f:\n",
    "        np.save(f, np.array(cam_int))\n",
    "    with open(f'../temp/camera_ext{mv_idx}.npy', 'wb') as f:\n",
    "        np.save(f, np.array(cam_ext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depth Anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "\n",
    "# pipeline = pipeline(\"depth-estimation\", model=\"depth-anything/Depth-Anything-V2-Large-hf\", device=0)\n",
    "# depth = pipe(image_DA)[\"depth\"]\n",
    "\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"depth-anything/Depth-Anything-V2-Large-hf\")\n",
    "# model = AutoModelForDepthEstimation.from_pretrained(\"depth-anything/Depth-Anything-V2-Large-hf\")\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"depth-anything/Depth-Anything-V2-Metric-Indoor-Large-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"depth-anything/Depth-Anything-V2-Metric-Indoor-Large-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_depth_list = []\n",
    "\n",
    "for i in range(3):\n",
    "    image_DA = Image.fromarray((img_views[0][i].transpose(1, 2, 0) * 255).astype(np.uint8))\n",
    "    inputs = image_processor(images=image_DA, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    post_processed_output = image_processor.post_process_depth_estimation(\n",
    "        outputs,\n",
    "        target_sizes=[(image_DA.height, image_DA.width)],\n",
    "    )\n",
    "\n",
    "    predicted_depth = post_processed_output[0][\"predicted_depth\"]\n",
    "    predicted_depth_list.append(predicted_depth.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(15, 7))\n",
    "for i in range(3):\n",
    "    ax[0, i].imshow(predicted_depth_list[i], cmap='plasma')  # Change to HxW for plotting\n",
    "    ax[1, i].imshow(depth_views[0][i], cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_flag_dump = False\n",
    "if _flag_dump:\n",
    "    with open(f'../temp/mv_img_views{mv_idx}.npy', 'wb') as f:\n",
    "        np.save(f, np.array(img_views))\n",
    "    with open(f'../temp/mv_depth_views{mv_idx}_DA.npy', 'wb') as f:\n",
    "        np.save(f, np.array(predicted_depth_list)[None,:])\n",
    "    with open(f'../temp/camera_int{mv_idx}.npy', 'wb') as f:\n",
    "        np.save(f, np.array(cam_int))\n",
    "    with open(f'../temp/camera_ext{mv_idx}.npy', 'wb') as f:\n",
    "        np.save(f, np.array(cam_ext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depth PRO Prediction (NOT A DEPTH COMPLETION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_apple_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import depth_pro\n",
    "\n",
    "if if_apple_model:\n",
    "    config = depth_pro.DEFAULT_MONODEPTH_CONFIG_DICT  # Changed init for this\n",
    "    config.checkpoint_uri = '/home/guests/tuna_gurbuz/prototype/models/ml-depth-pro/checkpoints/depth_pro.pt'\n",
    "    model, transform = depth_pro.create_model_and_transforms()\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if if_apple_model:\n",
    "    predicted_apple_depth_list = []\n",
    "    for idx, im in enumerate(img):\n",
    "        # Preprocess an image\n",
    "        if im.shape[-2] != gt_depth_image.shape[-2]:\n",
    "            print('REMOVE PADDING')\n",
    "            H,W = gt_depth_image.shape[-2:]\n",
    "            padding = (W-H) // 2\n",
    "            im = im[:, padding:-padding, :]\n",
    "        print(f'Image shape: {im.shape}')\n",
    "        image_transformed = transform(im.transpose(1,2,0))  # Transpose to (H, W, C) for PIL\n",
    "        f_px = torch.tensor(cam_int[idx][0, 0])  # Focal length in pixels (fx)\n",
    "\n",
    "        # Run inference\n",
    "        prediction = model.infer(image_transformed, f_px=f_px)\n",
    "        depth = prediction[\"depth\"]  # Depth in [mm].\n",
    "        focallength_px = prediction[\"focallength_px\"]  # Focal length in pixels.\n",
    "        predicted_apple_depth_list.append(depth.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth.max(), depth.min(), depth.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if if_apple_model:\n",
    "    scale_gt_depth_image = gt_depth_image[0]\n",
    "    pos = np.where(scale_gt_depth_image >= 100)  # 100 mm threshold\n",
    "    gt_y = scale_gt_depth_image[pos][:, None] / 1000.0  # Convert to meters\n",
    "    x = depth.cpu().numpy()[pos][:, None]\n",
    "\n",
    "    # Linear regression to find the scale factor\n",
    "    scaling_factor = np.linalg.inv(x.T @ x) @ (x.T @ gt_y)  # It is scaler so leave the np.eye\n",
    "    print(f'Scaling factor: {scaling_factor}')\n",
    "\n",
    "    # Apply scaling factor to the predicted depth\n",
    "    depth_scaled = depth.cpu().numpy() * scaling_factor[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(15, 7))\n",
    "for i in range(3):\n",
    "    ax[0, i].imshow(predicted_apple_depth_list[i], cmap='plasma')  # Change to HxW for plotting\n",
    "    ax[1, i].imshow(depth_views[0][i], cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_flag_dump = False\n",
    "if _flag_dump:\n",
    "    with open(f'../temp/mv_img_views{mv_idx}.npy', 'wb') as f:\n",
    "        np.save(f, np.array(img_views))\n",
    "    with open(f'../temp/mv_depth_views{mv_idx}_APPLE.npy', 'wb') as f:\n",
    "        np.save(f, np.array(predicted_apple_depth_list)[None,:])\n",
    "    with open(f'../temp/camera_int{mv_idx}.npy', 'wb') as f:\n",
    "        np.save(f, np.array(cam_int))\n",
    "    with open(f'../temp/camera_ext{mv_idx}.npy', 'wb') as f:\n",
    "        np.save(f, np.array(cam_ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_views[0].shape, depth_views[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "prompt_depth_url = \"https://github.com/DepthAnything/PromptDA/blob/main/assets/example_images/arkit_depth.png?raw=true\"\n",
    "prompt_depth = Image.open(requests.get(prompt_depth_url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"depth-anything/prompt-depth-anything-vitl-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"depth-anything/prompt-depth-anything-vitl-hf\")\n",
    "\n",
    "predicted_depth_list_PDA = []\n",
    "\n",
    "for i in range(3):\n",
    "    image_PDA = Image.fromarray((img_views[0][i].transpose(1, 2, 0) * 255).astype(np.uint8))\n",
    "    prompt_depth = depth_views[0][i] / 1000.0\n",
    "    prompt_depth = Image.fromarray(prompt_depth)\n",
    "    inputs = image_processor(images=image_PDA, return_tensors=\"pt\", prompt_depth=prompt_depth)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    post_processed_output = image_processor.post_process_depth_estimation(\n",
    "        outputs,\n",
    "        target_sizes=[(image_PDA.height, image_PDA.width)],\n",
    "    )\n",
    "\n",
    "    predicted_depth = post_processed_output[0][\"predicted_depth\"]\n",
    "    predicted_depth_list_PDA.append(predicted_depth.cpu().numpy())\n",
    "\n",
    "# visualize the prediction\n",
    "predicted_depth = post_processed_output[0][\"predicted_depth\"]\n",
    "depth = predicted_depth * 1000 \n",
    "depth = depth.detach().cpu().numpy()\n",
    "depth = Image.fromarray(depth.astype(\"uint16\")) # mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(15, 7))\n",
    "for i in range(3):\n",
    "    ax[0, i].imshow(predicted_depth_list_PDA[i] * 1000, cmap='plasma')  # Change to HxW for plotting\n",
    "    ax[1, i].imshow(depth_views[0][i], cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_flag_dump = False\n",
    "if _flag_dump:\n",
    "    with open(f'../temp/mv_img_views{mv_idx}.npy', 'wb') as f:\n",
    "        np.save(f, np.array(img_views))\n",
    "    with open(f'../temp/mv_depth_views{mv_idx}_PDA.npy', 'wb') as f:\n",
    "        np.save(f, np.array(predicted_depth_list_PDA)[None,:] * 1000)\n",
    "    with open(f'../temp/camera_int{mv_idx}.npy', 'wb') as f:\n",
    "        np.save(f, np.array(cam_int))\n",
    "    with open(f'../temp/camera_ext{mv_idx}.npy', 'wb') as f:\n",
    "        np.save(f, np.array(cam_ext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marigold DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import diffusers\n",
    "from diffusers import DDIMScheduler, MarigoldDepthPipeline\n",
    "\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "diffusers.utils.logging.disable_progress_bar()\n",
    "\n",
    "class MarigoldDepthCompletionPipeline(MarigoldDepthPipeline):\n",
    "    \"\"\"\n",
    "    Pipeline for Marigold Depth Completion.\n",
    "    Extends the MarigoldDepthPipeline to include depth completion functionality.\n",
    "    \"\"\"\n",
    "    def __call__(\n",
    "        self, image: Image.Image, sparse_depth: np.ndarray,\n",
    "        num_inference_steps: int = 50, processing_resolution: int = 768, seed: int = 2024\n",
    "    ) -> np.ndarray:\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image (PIL.Image.Image): Input image of shape [H, W] with 3 channels.\n",
    "            sparse_depth (np.ndarray): Sparse depth guidance of shape [H, W].\n",
    "            num_inference_steps (int, optional): Number of denoising steps. Defaults to 50.\n",
    "            processing_resolution (int, optional): Resolution for processing. Defaults to 768.\n",
    "            seed (int, optional): Random seed. Defaults to 2024.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Dense depth prediction of shape [H, W].\n",
    "\n",
    "        \"\"\"\n",
    "        # Resolving variables\n",
    "        device = self._execution_device\n",
    "        generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "        # Check inputs.\n",
    "        if num_inference_steps is None:\n",
    "            raise ValueError(\"Invalid num_inference_steps\")\n",
    "        if type(sparse_depth) is not np.ndarray or sparse_depth.ndim != 2:\n",
    "            raise ValueError(\"Sparse depth should be a 2D numpy ndarray with zeros at missing positions\")\n",
    "\n",
    "        # Prepare empty text conditioning\n",
    "        with torch.no_grad():\n",
    "            if self.empty_text_embedding is None:\n",
    "                text_inputs = self.tokenizer(\"\", padding=\"do_not_pad\", \n",
    "                    max_length=self.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "                text_input_ids = text_inputs.input_ids.to(device)\n",
    "                self.empty_text_embedding = self.text_encoder(text_input_ids)[0]  # [1,2,1024]\n",
    "\n",
    "        # Preprocess input images\n",
    "        image, padding, original_resolution = self.image_processor.preprocess(\n",
    "            image, processing_resolution=processing_resolution, device=device, dtype=self.dtype\n",
    "        )  # [N,3,PPH,PPW]\n",
    "\n",
    "        # Check sparse depth dimensions\n",
    "        if sparse_depth.shape != original_resolution:\n",
    "            raise ValueError(\n",
    "                f\"Sparse depth dimensions ({sparse_depth.shape}) must match that of the image ({image.shape[-2:]})\"\n",
    "            )\n",
    "        \n",
    "        # Encode input image into latent space\n",
    "        with torch.no_grad():\n",
    "            image_latent, pred_latent = self.prepare_latents(image, None, generator, 1, 1)  # [N*E,4,h,w], [N*E,4,h,w]\n",
    "        del image\n",
    "\n",
    "        # Preprocess sparse depth\n",
    "        sparse_depth = torch.from_numpy(sparse_depth)[None, None].float().to(device)\n",
    "        sparse_mask = sparse_depth > 0\n",
    "        logging.info(f\"Using {sparse_mask.int().sum().item()} guidance points\")\n",
    "\n",
    "        # Set up optimization targets and compute the range and lower bound of the sparse depth\n",
    "        scale, shift = torch.nn.Parameter(torch.ones(1, device=device)), torch.nn.Parameter(torch.ones(1, device=device))\n",
    "        pred_latent = torch.nn.Parameter(pred_latent)\n",
    "        sparse_range = (sparse_depth[sparse_mask].max() - sparse_depth[sparse_mask].min()).item() # (cmax âˆ’ cmin)\n",
    "        sparse_lower = (sparse_depth[sparse_mask].min()).item() # cmin\n",
    "        \n",
    "        # Set up optimizer\n",
    "        optimizer = torch.optim.Adam([ {\"params\": [scale, shift], \"lr\": 0.005},\n",
    "                                       {\"params\": [pred_latent] , \"lr\": 0.05 }])\n",
    "\n",
    "        def affine_to_metric(depth: torch.Tensor) -> torch.Tensor:\n",
    "            # Convert affine invariant depth predictions to metric depth predictions using the parametrized scale and shift. See Equation 2 of the paper.\n",
    "            return (scale**2) * sparse_range * depth + (shift**2) * sparse_lower\n",
    "\n",
    "        def latent_to_metric(latent: torch.Tensor) -> torch.Tensor:\n",
    "            # Decode latent to affine invariant depth predictions and subsequently to metric depth predictions.\n",
    "            affine_invariant_prediction = self.decode_prediction(latent)  # [E,1,PPH,PPW]\n",
    "            prediction = affine_to_metric(affine_invariant_prediction)\n",
    "            prediction = self.image_processor.unpad_image(prediction, padding)  # [E,1,PH,PW]\n",
    "            prediction = self.image_processor.resize_antialias(\n",
    "                prediction, original_resolution, \"bilinear\", is_aa=False\n",
    "            )  # [1,1,H,W]\n",
    "            return prediction\n",
    "\n",
    "        def loss_l1l2(input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "            # Compute L1 and L2 loss between input and target.\n",
    "            out_l1 = torch.nn.functional.l1_loss(input, target)\n",
    "            out_l2 = torch.nn.functional.mse_loss(input, target)\n",
    "            out = out_l1 + out_l2\n",
    "            return out\n",
    "\n",
    "        # Denoising loop\n",
    "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "        for _, t in enumerate(\n",
    "            self.progress_bar(self.scheduler.timesteps, desc=f\"Marigold-DC steps ({str(device)})...\")\n",
    "        ):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through the U-Net\n",
    "            batch_latent = torch.cat([image_latent, pred_latent], dim=1)  # [1,8,h,w]\n",
    "            noise = self.unet(\n",
    "                batch_latent, t, encoder_hidden_states=self.empty_text_embedding, return_dict=False\n",
    "            )[0]  # [1,4,h,w]\n",
    "\n",
    "            # Compute pred_epsilon to later rescale the depth latent gradient\n",
    "            with torch.no_grad():\n",
    "                alpha_prod_t = self.scheduler.alphas_cumprod[t]\n",
    "                beta_prod_t = 1 - alpha_prod_t\n",
    "                pred_epsilon = (alpha_prod_t**0.5) * noise + (beta_prod_t**0.5) * pred_latent\n",
    "\n",
    "            step_output = self.scheduler.step(noise, t, pred_latent, generator=generator)\n",
    "\n",
    "            # Preview the final output depth with Tweedie's formula (See Equation 1 of the paper)\n",
    "            pred_original_sample = step_output.pred_original_sample\n",
    "\n",
    "            # Decode to metric space, compute loss with guidance and backpropagate\n",
    "            current_metric_estimate = latent_to_metric(pred_original_sample)\n",
    "            loss = loss_l1l2(current_metric_estimate[sparse_mask], sparse_depth[sparse_mask])\n",
    "            loss.backward()\n",
    "\n",
    "            # Scale gradients up\n",
    "            with torch.no_grad():\n",
    "                pred_epsilon_norm = torch.linalg.norm(pred_epsilon).item()\n",
    "                depth_latent_grad_norm = torch.linalg.norm(pred_latent.grad).item()\n",
    "                scaling_factor = pred_epsilon_norm / max(depth_latent_grad_norm, 1e-8)\n",
    "                pred_latent.grad *= scaling_factor\n",
    "\n",
    "            # Execute the update step through guidance backprop\n",
    "            optimizer.step()\n",
    "\n",
    "            # Execute update of the latent with regular denoising diffusion step\n",
    "            with torch.no_grad():\n",
    "                pred_latent.data = self.scheduler.step(noise, t, pred_latent, generator=generator).prev_sample\n",
    "\n",
    "            del pred_original_sample, current_metric_estimate, step_output, pred_epsilon, noise\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        del image_latent\n",
    "\n",
    "        # Decode predictions from latent into pixel space\n",
    "        with torch.no_grad():\n",
    "            prediction = latent_to_metric(pred_latent.detach())\n",
    "\n",
    "        # return Numpy array\n",
    "        prediction = self.image_processor.pt_to_numpy(prediction)  # [N,H,W,1]\n",
    "        self.maybe_free_model_hooks()\n",
    "\n",
    "        return prediction.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH_CHECKPOINT = \"prs-eth/marigold-depth-v1-0\"\n",
    "num_inference_steps = 100  # 50\n",
    "processing_resolution = 768\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    processing_resolution_non_cuda = 512\n",
    "    num_inference_steps_non_cuda = 10\n",
    "    if processing_resolution > processing_resolution_non_cuda:\n",
    "        logging.warning(f\"CUDA not found: Reducing processing_resolution to {processing_resolution_non_cuda}\")\n",
    "        processing_resolution = processing_resolution_non_cuda\n",
    "    if num_inference_steps > num_inference_steps_non_cuda:\n",
    "        logging.warning(f\"CUDA not found: Reducing num_inference_steps to {num_inference_steps_non_cuda}\")\n",
    "        num_inference_steps = num_inference_steps_non_cuda\n",
    "\n",
    "pipe = MarigoldDepthCompletionPipeline.from_pretrained(DEPTH_CHECKPOINT, prediction_type=\"depth\").to(device)\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    logging.warning(\"CUDA not found: Using a lightweight VAE\")\n",
    "    del pipe.vae\n",
    "    pipe.vae = diffusers.AutoencoderTiny.from_pretrained(\"madebyollin/taesd\").to(device)\n",
    "\n",
    "pred_list_marigold = []\n",
    "convert_pil = ToPILImage()\n",
    "for i in range(3):\n",
    "    pred = pipe(\n",
    "        image=convert_pil(img_views[0][i].transpose(1, 2, 0)),\n",
    "        sparse_depth=gt_depth_image[i] / 1000.0,  # Convert it to meters\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        processing_resolution=processing_resolution,\n",
    "    )\n",
    "    pred_list_marigold.append(pred)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(15, 7))\n",
    "for i in range(3):\n",
    "    ax[0, i].imshow(pred_list_marigold[i] * 1000, cmap='plasma')  # Change to HxW for plotting\n",
    "    ax[1, i].imshow(depth_views[0][i], cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least Squares Fit for the prediction metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    y_depth = gt_depth_image[i]\n",
    "    pos = np.where(y_depth >= 200)  # 100 mm threshold\n",
    "    gt_y = y_depth[pos][:, None] / 1000.0\n",
    "    x = pred[pos][:, None]\n",
    "\n",
    "    # Linear regression to find the scale factor\n",
    "    scaling_factor = np.linalg.inv(x.T @ x) @ (x.T @ gt_y)  # It is scaler so leave the np.eye\n",
    "    print(f'Scaling factor: {scaling_factor}')\n",
    "\n",
    "    # Apply scaling factor to the predicted depth\n",
    "    pred_scaled = pred * scaling_factor[0, 0]\n",
    "\n",
    "    # Check MSE\n",
    "    se = (x * scaling_factor - gt_y) ** 2\n",
    "    mse = np.mean(se)\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    print(f'Max Error: {se.max()}')\n",
    "    pos_max = se.argmax()\n",
    "    print(f'Max Error Position: {pos_max}, Predicted: {x[pos_max]}, GT: {gt_y[pos_max]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_flag_dump = False\n",
    "if _flag_dump:\n",
    "    with open(f'../temp/mv_img_views{mv_idx}.npy', 'wb') as f:\n",
    "        np.save(f, np.array(img_views))\n",
    "    with open(f'../temp/mv_depth_views{mv_idx}_MarigoldDC.npy', 'wb') as f:\n",
    "        np.save(f, np.array(pred_list_marigold)[None,:])\n",
    "    with open(f'../temp/camera_int{mv_idx}.npy', 'wb') as f:\n",
    "        np.save(f, np.array(cam_int))\n",
    "    with open(f'../temp/camera_ext{mv_idx}.npy', 'wb') as f:\n",
    "        np.save(f, np.array(cam_ext))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
