{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd941407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "\n",
    "project_dir = os.path.dirname(os.getcwd())\n",
    "print(project_dir)\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import ToPILImage, ToTensor, Normalize, Resize\n",
    "from training.dataset.transforms import ComposeAPI, NormalizeAPI\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset.collate_fn import collate_fn\n",
    "from dataset.mini_dataset import MiniDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b843e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "object_labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 24, 25, 26, 27, 28, 29]\n",
    "len_video = 1\n",
    "input_image_size = 512\n",
    "batch_size = 1\n",
    "shuffle = False\n",
    "multiview = True\n",
    "mean = [0.3551, 0.3500, 0.3469]\n",
    "std = [0.2921, 0.2716, 0.2742]\n",
    "transforms = [ComposeAPI([NormalizeAPI(mean=mean, std=std, v2=True)])]\n",
    "revert_mean=[-.3551/.2921, -.3500/.2716, -.3469/.2742]\n",
    "revert_std=[1/.2921, 1/.2716, 1/.2742]\n",
    "revert_transform = Normalize(mean=revert_mean, std=revert_std)\n",
    "test_dataset = MiniDataset('train',\n",
    "                           num_frames=len_video,\n",
    "                           input_image_size=input_image_size,\n",
    "                           object_labels=object_labels,\n",
    "                           transforms=transforms,\n",
    "                           collate_fn=collate_fn,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=shuffle,\n",
    "                           multiview=multiview,)\n",
    "print(f'Lenght of the dataset! {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_int_ext = [(mtrx[0].numpy(), mtrx[1].numpy()) for mtrx in test_dataset.camera_features['001_PKA']]\n",
    "cam_int = [mtrx[0] for mtrx in cam_int_ext]\n",
    "cam_ext = [mtrx[1] for mtrx in cam_int_ext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(mv_idx, img_indices):\n",
    "    # Get image path\n",
    "    img_paths = test_dataset.images[img_indices, :].squeeze(1)  # get image paths for the multiview indices\n",
    "    # Load video data batch\n",
    "    video_data_batch = test_dataset[mv_idx]\n",
    "    # Inverse mean std normalization\n",
    "    img0 = revert_transform(video_data_batch[0][0].frames[0].data).numpy()\n",
    "    img1 = revert_transform(video_data_batch[0][1].frames[0].data).numpy()\n",
    "    img2 = revert_transform(video_data_batch[0][2].frames[0].data).numpy()\n",
    "    img = (img0, img1, img2)\n",
    "    # Depth path\n",
    "    depth_images = []\n",
    "    for img_path in img_paths:\n",
    "        last_part = str(img_path.parts[-1].split('/')[-1].replace('.jpg', '.tiff').replace('color', 'depth'))\n",
    "        depth_path = img_path.parents[1] / 'depthimage' / last_part\n",
    "        # Load depth image\n",
    "        gt_depth_image = Image.open(depth_path)\n",
    "        # Resize depth image to 512x512\n",
    "        gt_depth_image = gt_depth_image.resize((gt_depth_image.size[0]//2, gt_depth_image.size[1]//2))\n",
    "        depth_images.append(np.array(gt_depth_image))\n",
    "    return img, depth_images, depth_path, video_data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dataset = len(test_dataset)\n",
    "print(f'Length of the dataset: {len_dataset}')\n",
    "mv_indices = [1115]  # [700, 1115, 1442]\n",
    "img_views = []\n",
    "depth_views = []\n",
    "\n",
    "for mv_idx in tqdm(mv_indices):\n",
    "    # Get image path and camera index\n",
    "    img_indices = [mv_idx * 3 + kk for kk in range(3)]\n",
    "    img, gt_depth_image, depth_path, video_data_batch = process_img(mv_idx, img_indices)\n",
    "    img = np.stack(img, axis=0)  # Stack images along a new dimension\n",
    "    gt_depth_image = np.stack(gt_depth_image, axis=0)  # Stack depth images along a new dimension\n",
    "    H,W = gt_depth_image.shape[-2:]\n",
    "    buffer = (W-H) // 2\n",
    "    img = img[:,:, buffer:-buffer, :]\n",
    "    assert img.shape[-2:] == gt_depth_image.shape[-2:]\n",
    "    img_views.append(img)\n",
    "    depth_views.append(gt_depth_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebc1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(15, 7))\n",
    "for i, im in enumerate(img):\n",
    "    axs[0, i].imshow(im.transpose(1, 2, 0))  # Change to HxWxC for plotting\n",
    "    axs[0, i].set_title(f'Image {i+1}')\n",
    "    axs[0, i].axis('off')\n",
    "    \n",
    "    axs[1, i].imshow(gt_depth_image[i], cmap='plasma')  # Change to HxW for plotting\n",
    "    axs[1, i].set_title(f'Depth {i+1}')\n",
    "    axs[1, i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f970d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_id = 9\n",
    "mask0 = video_data_batch[0][0].frames[0].objects[obj_id].segment\n",
    "mask1 = video_data_batch[0][1].frames[0].objects[obj_id].segment\n",
    "mask2 = video_data_batch[0][2].frames[0].objects[obj_id].segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269fb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(15, 7))\n",
    "axs[0, 0].imshow(mask0.squeeze().numpy(), cmap='gray')\n",
    "axs[0, 1].imshow(mask1.squeeze().numpy(), cmap='gray')\n",
    "axs[0, 2].imshow(mask2.squeeze().numpy(), cmap='gray')\n",
    "axs[0, 0].set_title('Mask 0')\n",
    "axs[0, 1].set_title('Mask 1')\n",
    "axs[0, 2].set_title('Mask 2')\n",
    "\n",
    "axs[1, 0].imshow(gt_depth_image[0], cmap='plasma')  # Change to HxW for plotting\n",
    "axs[1, 1].imshow(gt_depth_image[1], cmap='plasma')  # Change to HxW for plotting\n",
    "axs[1, 2].imshow(gt_depth_image[2], cmap='plasma')  # Change to HxW for plotting\n",
    "axs[1, 0].set_title('Depth 1')\n",
    "axs[1, 1].set_title('Depth 2')\n",
    "axs[1, 2].set_title('Depth 3')\n",
    "axs[1, 0].axis('off')\n",
    "axs[1, 1].axis('off')\n",
    "axs[1, 2].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5815504",
   "metadata": {},
   "source": [
    "Cropped Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = 64\n",
    "depth_trunc = 10 * 1e3\n",
    "\n",
    "work_on_cropped_image = False\n",
    "if work_on_cropped_image:\n",
    "    cropped_mask0 = mask0.squeeze().numpy()[padding:-padding]\n",
    "    cropped_mask1 = mask1.squeeze().numpy()[padding:-padding]\n",
    "    cropped_mask2 = mask2.squeeze().numpy()[padding:-padding]\n",
    "\n",
    "    y0_, x0_ = np.where(cropped_mask0 > 0)\n",
    "    y1_, x1_ = np.where(cropped_mask1 > 0)\n",
    "    y2_, x2_ = np.where(cropped_mask2 > 0)\n",
    "\n",
    "    mask0_depth = gt_depth_image[0].squeeze()[y0_, x0_]\n",
    "    mask1_depth = gt_depth_image[1].squeeze()[y1_, x1_]\n",
    "    mask2_depth = gt_depth_image[2].squeeze()[y2_, x2_]\n",
    "\n",
    "    mask0_depth[mask0_depth > depth_trunc] = depth_trunc\n",
    "    mask1_depth[mask1_depth > depth_trunc] = depth_trunc\n",
    "    mask2_depth[mask2_depth > depth_trunc] = depth_trunc\n",
    "else:\n",
    "    cropped_mask0 = mask0.squeeze().numpy()\n",
    "    cropped_mask1 = mask1.squeeze().numpy()\n",
    "    cropped_mask2 = mask2.squeeze().numpy()\n",
    "\n",
    "    y0_, x0_ = np.where(cropped_mask0 > 0)\n",
    "    y1_, x1_ = np.where(cropped_mask1 > 0)\n",
    "    y2_, x2_ = np.where(cropped_mask2 > 0)\n",
    "    \n",
    "    mask0_depth = np.zeros_like(cropped_mask0, dtype=float)\n",
    "    mask1_depth = np.zeros_like(cropped_mask1, dtype=float)\n",
    "    mask2_depth = np.zeros_like(cropped_mask2, dtype=float)\n",
    "\n",
    "    mask0_depth[padding:-padding] = gt_depth_image[0]\n",
    "    mask1_depth[padding:-padding] = gt_depth_image[1]\n",
    "    mask2_depth[padding:-padding] = gt_depth_image[2]\n",
    "\n",
    "    mask0_depth = mask0_depth.squeeze()[y0_, x0_]\n",
    "    mask1_depth = mask1_depth.squeeze()[y1_, x1_]\n",
    "    mask2_depth = mask2_depth.squeeze()[y2_, x2_]\n",
    "\n",
    "    mask0_depth[mask0_depth > depth_trunc] = depth_trunc\n",
    "    mask1_depth[mask1_depth > depth_trunc] = depth_trunc\n",
    "    mask2_depth[mask2_depth > depth_trunc] = depth_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f476f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "K0, K1, K2 = cam_int[0], cam_int[1], cam_int[2]\n",
    "T0, T1, T2 = cam_ext[0], cam_ext[1], cam_ext[2]  # Transformations are from camera to world coordinates\n",
    "\n",
    "if work_on_cropped_image:\n",
    "    K0[1,2] -= padding\n",
    "    K1[1,2] -= padding\n",
    "    K2[1,2] -= padding\n",
    "\n",
    "print(f'Intrinsics: {K0},\\n{K1},\\n{K2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6608999f",
   "metadata": {},
   "source": [
    "DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug0 = np.zeros_like(cropped_mask0)\n",
    "# debug1 = np.zeros_like(cropped_mask1)\n",
    "# debug2 = np.zeros_like(cropped_mask2)\n",
    "\n",
    "# debug0[y0_, x0_] = mask0_depth\n",
    "# debug1[y1_, x1_] = mask1_depth\n",
    "# debug2[y2_, x2_] = mask2_depth\n",
    "\n",
    "# plt.imshow(debug0, cmap='plasma')\n",
    "# plt.imshow(debug1, cmap='plasma')\n",
    "# plt.imshow(debug2, cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44883140",
   "metadata": {},
   "source": [
    "Point Cloud Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21121446",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_coords0 = np.stack([x0_, y0_, np.ones_like(x0_)], axis=1)  # Homogeneous coordinates Nx3\n",
    "h_coords1 = np.stack([x1_, y1_, np.ones_like(x1_)], axis=1)  # Homogeneous coordinates Nx3\n",
    "h_coords2 = np.stack([x2_, y2_, np.ones_like(x2_)], axis=1)  # Homogeneous coordinates Nx3\n",
    "\n",
    "im_coords0 = np.linalg.inv(K0) @ h_coords0.T  # Inverse camera matrix multiplication 3xN\n",
    "im_coords1 = np.linalg.inv(K1) @ h_coords1.T  # Inverse camera matrix multiplication 3xN\n",
    "im_coords2 = np.linalg.inv(K2) @ h_coords2.T  # Inverse camera matrix multiplication 3xN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_coords0_depth_scaled = im_coords0 * mask0_depth / 1000.0\n",
    "im_coords1_depth_scaled = im_coords1 * mask1_depth / 1000.0\n",
    "im_coords2_depth_scaled = im_coords2 * mask2_depth / 1000.0\n",
    "\n",
    "obj_pc0 = T0 @ np.concatenate([im_coords0_depth_scaled, np.ones_like(x0_)[None,:]], axis=0)  # Transforming to world coordinates 4xN\n",
    "obj_pc1 = T1 @ np.concatenate([im_coords1_depth_scaled, np.ones_like(x1_)[None,:]], axis=0)  # Transforming to world coordinates 4xN\n",
    "obj_pc2 = T2 @ np.concatenate([im_coords2_depth_scaled, np.ones_like(x2_)[None,:]], axis=0)  # Transforming to world coordinates 4xN\n",
    "\n",
    "# obj_pc = np.concatenate([obj_pc0, obj_pc1, obj_pc2], axis=1)\n",
    "obj_pc = np.concatenate([obj_pc1, obj_pc2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_flag_dump = False\n",
    "if _flag_dump:\n",
    "    with open(f'../temp/pc_test_obj_{obj_id}.npy', 'wb') as f:\n",
    "        np.save(f, np.array(obj_pc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17413c32",
   "metadata": {},
   "source": [
    "Projection to 1st Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a2df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_pts0 = K0 @ (np.linalg.inv(T0) @ obj_pc)[:3, :]  # Projecting back to image coordinates 3xN\n",
    "normalized_pts0 = projected_pts0 / projected_pts0[2, :]\n",
    "pixel_pts0 = normalized_pts0[:2, :]  # Pixel coordinates 2xN\n",
    "pixel_pts0 = pixel_pts0.round().astype(int)  # Round to nearest integer for pixel coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7497df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = cropped_mask0.shape\n",
    "mask = np.zeros([H, W])\n",
    "refinement = (pixel_pts0[1] > (H-1)) | (pixel_pts0[1] < 0) | (pixel_pts0[0] > (W-1)) | (pixel_pts0[0] < 0)\n",
    "pixel_pts0[:, refinement] = 0  # Set invalid pixel coordinates to 0\n",
    "mask[pixel_pts0[1], pixel_pts0[0]] = 1  # Create a mask for the first image\n",
    "plt.imshow(mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ff35ff",
   "metadata": {},
   "source": [
    "Test Point Cloud Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52fe5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_models.models.sam2former.point_cloud_mask import point_cloud_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f5aba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_depth_image_test = [torch.tensor(gt_depth_image[0][None,:]), torch.tensor(gt_depth_image[1][None,:]), torch.tensor(gt_depth_image[2][None,:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2786f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_int_ext = test_dataset.camera_features['001_PKA']\n",
    "pts_cam0 = torch.tensor(np.stack([x0_, y0_], axis=1))\n",
    "pts_cam1 = torch.tensor(np.stack([x1_, y1_], axis=1))\n",
    "pts_cam2 = torch.tensor(np.stack([x2_, y2_], axis=1))\n",
    "epi_mask_for_views = point_cloud_mask(camera_int_ext, pts_cam0, pts_cam1, pts_cam2, gt_depth_image_test, [H,W])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask = torch.zeros(H,W)\n",
    "test_pixel = epi_mask_for_views[0][0]\n",
    "test_mask[test_pixel[1], test_pixel[0]] = 0.5  # Create a mask for the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df0878",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_mask, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
