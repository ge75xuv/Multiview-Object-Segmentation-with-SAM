# @package _global_

scratch:
  resolution: 518
  num_sampling_points: 4096  # 1024, 4096
  train_batch_size: 1
  num_train_workers: 0
  base_lr: 3.0e-4  # 5.0e-6
  obj_labels: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 24, 25, 26, 27, 28, 29]
  obj_ids_extra_sampling: [2, 12, 13, 14]  # extra object ids to sample from for accuracy
  num_frames: 1
  multiview: true
  depth_image: false
  vggt_dataset: true
  max_epochs: 70
  # Decoder
  num_queries: 23
  num_classes: 23
  use_dpt_weights: False
  label_projection_type: 'default'  # 'default' or 'human'
  
  mask_dim: 256
  name: ["res3", "res4", "res5"]
  channels: [256, 256, 256]
  # Stride and common stride determines the extra fpn levels NOTE
  stride: [4, 4, 4]
  old_dec_layers: true

# Video transforms
vos:
  train_transforms:
    - _target_: training.dataset.transforms.ComposeAPI
      transforms:
        - _target_: torch.nn.Identity

trainer:
  _target_: custom_models.trainer.Trainer
  mode: train  # [train]
  max_epochs: ${scratch.max_epochs}
  accelerator: cuda
  seed_value: 123

  model:
    _target_: custom_models.models.vggtformer.vggt_former_base.VGGT
    img_size: 518
    patch_size: 14
    embed_dim: 1024
    num_classes: ${scratch.num_classes}

    # Decoder
    mask_decoder_cfg:
      # _target_: custom_models.models.sam2former.mask_former_head.MaskFormerHead
      name: ${scratch.name}
      channels: ${scratch.channels}
      stride: ${scratch.stride}
      num_classes: ${scratch.num_classes}
      # Pixel Decoder
      pixel_decoder:
        target_holder: custom_models.models.sam2former.msdeformattn.MSDeformAttnPixelDecoder
        # input_shape: ${scratch.input_shape}
        transformer_dropout: 0.0
        transformer_nheads: 8
        transformer_dim_feedforward: 2048
        transformer_enc_layers: 6
        conv_dim: 256
        mask_dim: ${scratch.mask_dim}
        norm: "GN"
        # deformable transformer encoder args
        transformer_in_features: ${scratch.name}
        common_stride: 4
      loss_weight: 1.0
      ignore_value: 255
      transformer_predictor:
        target_holder: custom_models.models.sam2former.mask2former_transformer_decoder.MultiScaleMaskedTransformerDecoder
        in_channels: 256
        num_classes: ${scratch.num_classes}
        hidden_dim: 256
        num_queries: ${scratch.num_queries}
        nheads: 8
        dim_feedforward: 2048
        dec_layers: 10
        old_dec_layers: ${scratch.old_dec_layers}
        pre_norm: false
        mask_dim: ${scratch.mask_dim}
        enforce_input_project: false
      transformer_in_feature: 'multi_scale_pixel_decoder'
    
  data:
    train:
      _target_: custom_models.dataset.mini_dataset_vggt.MiniDatasetVGGT
      batch_size: ${scratch.train_batch_size}
      split_type: 'train'
      num_frames: ${scratch.num_frames}
      input_image_size: ${scratch.resolution}
      object_labels: ${scratch.obj_labels}
      label_projection_type: ${scratch.label_projection_type}
      num_workers: ${scratch.num_train_workers}
      multiview: ${scratch.multiview}
      collate_fn:
        # _target_: training.utils.data_utils.collate_fn
        _target_: custom_models.dataset.collate_fn.collate_fn_wrapper
        _partial_: true
        num_frames: ${scratch.num_frames}
        dict_key: all
    val:
      _target_: custom_models.dataset.mini_dataset_vggt.MiniDatasetVGGT
      batch_size: ${scratch.train_batch_size}
      split_type: 'val'
      num_frames: ${scratch.num_frames}
      input_image_size: ${scratch.resolution}
      object_labels: ${scratch.obj_labels}
      label_projection_type: ${scratch.label_projection_type}
      num_workers: ${scratch.num_train_workers}
      multiview: ${scratch.multiview}
      collate_fn:
        # _target_: training.utils.data_utils.collate_fn
        _target_: custom_models.dataset.collate_fn.collate_fn_wrapper
        _partial_: true
        num_frames: ${scratch.num_frames}
        dict_key: val

  optim:
    amp:
      enabled: True
      amp_dtype: bfloat16

    optimizer:
      _target_: torch.optim.AdamW

    gradient_clip:
      _target_: training.optimizer.GradientClipper
      max_norm: 0.1  # 0.01  # 0.1
      norm_type: 2

    # param_group_modifiers:
    #   - _target_: training.optimizer.layer_decay_param_modifier
    #     _partial_: True
    #     layer_decay_value: 0.9
    #     apply_to: 'image_encoder.trunk'
    #     overrides:
    #       - pattern: '*pos_embed*'
    #         value: 1.0

    options:
      lr:
        # - scheduler:
        #     _target_: custom_models.models.sam2former.lib.LinearParamScheduler
        #     # scheduler:
        #     #   _target_: fvcore.common.param_scheduler.CosineParamScheduler
        #     #   start_value: ${scratch.base_lr}
        #     #   end_value: ${divide:${scratch.base_lr},1}
        #     start_value: ${divide:${scratch.base_lr},1000}
        #     end_value:  ${scratch.base_lr}
        #     warmup_length: 0.05  # 5% of total steps, adjust as needed
        #   param_names:
        #     - "sam_mask_decoder.*"
        - scheduler:
            _target_: fvcore.common.param_scheduler.CosineParamScheduler
            start_value: ${scratch.base_lr}
            end_value: ${divide:${scratch.base_lr},1}
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.05  # 0.05 # 0.1
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names:
            - '*bias*'
          module_cls_names: ['torch.nn.LayerNorm']

  loss:
    all:
      _target_: custom_models.models.vggtformer.criterion.SetCriterion
      num_classes: ${scratch.num_classes}
      weight_dict:
        loss_mask: 5.0
        # loss_dice: 5.0
        # loss_class: 2.0
      eos_coef: 0.1
      alpha: -1
      loss_weighting: log  # HEADS UP: log linear or default
      losses: ['pixelwise_mask']  # Dont forget to change the weight dict too
      num_points: ${scratch.num_sampling_points}
      obj_ids_extra_sampling: ${scratch.obj_ids_extra_sampling}
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
      image_size: ${scratch.resolution}
      deep_supervision: False
      pointwise_mask: False
    val:
      _target_: custom_models.models.vggtformer.criterion.SetCriterion
      num_classes: ${scratch.num_classes}
      weight_dict:
        loss_mask: 5.0
        # loss_dice: 5.0
        # loss_iou: 1
        # loss_class: 2.0
      eos_coef: 0.1
      alpha: -1
      loss_weighting: log  # HEADS UP: log linear or default
      losses: ['pixelwise_mask']  # Dont forget to change the weight dict too
      num_points: ${scratch.num_sampling_points}
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
      image_size: ${scratch.resolution}
      deep_supervision: False
      pointwise_mask: False
  
  distributed:
    backend: nccl
    find_unused_parameters: False

  logging:
    tensorboard_writer:
      _target_: training.utils.logger.make_tensorboard_logger
      log_dir:  ${launcher.experiment_log_dir}/tensorboard
      flush_secs: 120
      should_log: True
    log_dir: ${launcher.experiment_log_dir}/logs
    log_freq: 10

  # initialize from a SAM 2 checkpoint
  checkpoint:
    # resume_from: sam2_logs/09_01_19_49/checkpoints/checkpoint_1.pt
    save_freq: 1 # 0 only last checkpoint is saved.
    save_dir: ${launcher.experiment_log_dir}/checkpoints
    model_weight_initializer:
      _partial_: True
      _target_: custom_models.models.sam2former.lib.load_state_dict_into_model_vggt
      strict: True
      use_dpt_weights: ${scratch.use_dpt_weights}
      state_dict:
        _target_: training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels
        checkpoint_path: /home/guests/tuna_gurbuz/prototype/models/vggt/checkpoints/model.pt
        # checkpoint_path: /home/guests/tuna_gurbuz/prototype/models/sam2/checkpoints/sam2.1_hiera_base_plus.pt # PATH to SAM 2.1 checkpoint
        ckpt_state_dict_keys: []

launcher:
  num_nodes: 1
  gpus_per_node: 1
  experiment_log_dir: null # Path to log directory, defaults to ./sam2_logs/${config_name}

# SLURM args if running on a cluster
submitit:
  partition: null
  account: null
  qos: null
  cpus_per_task: 10
  use_cluster: false
  timeout_hour: 24
  name: null
  port_range: [10000, 65000]

