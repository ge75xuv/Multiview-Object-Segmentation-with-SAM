# @package _global_

scratch:
  resolution: 518
  num_sampling_points: 4096  # 1024, 4096
  train_batch_size: 1
  num_train_workers: 1
  base_lr: 1e-5 # 1e-4 # 1.0e-5   # CHECK 
  obj_labels: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 24, 25, 26, 27, 28, 29]
  obj_ids_extra_sampling: [2, 12, 13, 14]  # extra object ids to sample from for accuracy
  num_frames: 1
  multiview: true
  all_cameras: false  # CHECK
  depth_image: false
  vggt_dataset: true
  max_epochs: 70
  # Decoder
  num_queries: 23
  num_classes: 23
  use_dpt_weights: false  # CHECK
  pre_trained_for_seg: false  # if true, loads the weights that were pre-trained for segmentation (not depth)
  label_projection_type: 'default'  # 'default' or 'human'
  # losses: ['pixelwise_mask']  # losses: ['masks']
  losses: ['masks']  # CHECK

  # Unfreeze settings
  number_of_last_dino_layers_to_unfreeze: 0  # CHECK
  number_of_first_agg_layers_to_unfreeze: 0  # CHECK

  # Apply Dropout in DPT head (This contradicts the original implementation)
  apply_dropout: true

  mask_dim: 256
  name: ["res3", "res4", "res5"]
  channels: [256, 256, 256]
  # Stride and common stride determines the extra fpn levels NOTE
  stride: [4, 4, 4]
  old_dec_layers: true

# Video transforms
vos:
  train_transforms:
    - _target_: training.dataset.transforms.ComposeAPI
      transforms:
        - _target_: torch.nn.Identity

trainer:
  _target_: custom_models.trainer.Trainer
  mode: train  # [train]
  max_epochs: ${scratch.max_epochs}
  accelerator: cuda
  seed_value: 123

  model:
    _target_: custom_models.models.vggtformer.vggt_former_base.VGGT
    img_size: 518
    patch_size: 14
    embed_dim: 1024
    num_classes: ${scratch.num_classes}
    all_cameras: ${scratch.all_cameras}
    loss_type: ${scratch.losses[0]}  # 'masks' or 'pixelwise_mask'
    number_of_last_dino_layers_to_unfreeze: ${scratch.number_of_last_dino_layers_to_unfreeze}
    number_of_first_agg_layers_to_unfreeze: ${scratch.number_of_first_agg_layers_to_unfreeze}
    apply_dropout: ${scratch.apply_dropout}

    # Decoder
    # mask_decoder_cfg:
    #   # _target_: custom_models.models.sam2former.mask_former_head.MaskFormerHead
    #   name: ${scratch.name}
    #   channels: ${scratch.channels}
    #   stride: ${scratch.stride}
    #   num_classes: ${scratch.num_classes}
    #   # Pixel Decoder
    #   pixel_decoder:
    #     target_holder: custom_models.models.sam2former.msdeformattn.MSDeformAttnPixelDecoder
    #     # input_shape: ${scratch.input_shape}
    #     transformer_dropout: 0.0
    #     transformer_nheads: 8
    #     transformer_dim_feedforward: 2048
    #     transformer_enc_layers: 6
    #     conv_dim: 256
    #     mask_dim: ${scratch.mask_dim}
    #     norm: "GN"
    #     # deformable transformer encoder args
    #     transformer_in_features: ${scratch.name}
    #     common_stride: 4
    #   loss_weight: 1.0
    #   ignore_value: 255
    #   transformer_predictor:
    #     target_holder: custom_models.models.sam2former.mask2former_transformer_decoder.MultiScaleMaskedTransformerDecoder
    #     in_channels: 256
    #     num_classes: ${scratch.num_classes}
    #     hidden_dim: 256
    #     num_queries: ${scratch.num_queries}
    #     nheads: 8
    #     dim_feedforward: 2048
    #     dec_layers: 10
    #     old_dec_layers: ${scratch.old_dec_layers}
    #     pre_norm: false
    #     mask_dim: ${scratch.mask_dim}
    #     enforce_input_project: false
    #   transformer_in_feature: 'multi_scale_pixel_decoder'
    
  data:
    train:
      _target_: custom_models.dataset.mini_dataset_vggt.MiniDatasetVGGT
      batch_size: ${scratch.train_batch_size}
      split_type: 'train'
      num_frames: ${scratch.num_frames}
      input_image_size: ${scratch.resolution}
      object_labels: ${scratch.obj_labels}
      label_projection_type: ${scratch.label_projection_type}
      num_workers: ${scratch.num_train_workers}
      all_cameras: ${scratch.all_cameras}
      multiview: ${scratch.multiview}
      collate_fn:
        # _target_: training.utils.data_utils.collate_fn
        _target_: custom_models.dataset.collate_fn.collate_fn_wrapper
        _partial_: true
        num_frames: ${scratch.num_frames}
        dict_key: all
    val:
      _target_: custom_models.dataset.mini_dataset_vggt.MiniDatasetVGGT
      batch_size: ${scratch.train_batch_size}
      split_type: 'val'
      num_frames: ${scratch.num_frames}
      input_image_size: ${scratch.resolution}
      object_labels: ${scratch.obj_labels}
      label_projection_type: ${scratch.label_projection_type}
      num_workers: ${scratch.num_train_workers}
      multiview: ${scratch.multiview}
      all_cameras: ${scratch.all_cameras}
      collate_fn:
        # _target_: training.utils.data_utils.collate_fn
        _target_: custom_models.dataset.collate_fn.collate_fn_wrapper
        _partial_: true
        num_frames: ${scratch.num_frames}
        dict_key: val

  optim:
    amp:
      enabled: True
      amp_dtype: bfloat16

    optimizer:
      _target_: torch.optim.AdamW

    gradient_clip:
      _target_: training.optimizer.GradientClipper
      max_norm: 0.1  # 0.01  # 0.1
      norm_type: 2

    # param_group_modifiers:
    #   - _target_: training.optimizer.layer_decay_param_modifier
    #     _partial_: True
    #     layer_decay_value: 0.9
    #     apply_to: 'image_encoder.trunk'
    #     overrides:
    #       - pattern: '*pos_embed*'
    #         value: 1.0

    options:
      lr:
        # - scheduler:
        #     _target_: custom_models.models.sam2former.lib.LinearParamScheduler
        #     # scheduler:
        #     #   _target_: fvcore.common.param_scheduler.CosineParamScheduler
        #     #   start_value: ${scratch.base_lr}
        #     #   end_value: ${divide:${scratch.base_lr},1}
        #     start_value: ${divide:${scratch.base_lr},1000}
        #     end_value:  ${scratch.base_lr}
        #     warmup_length: 0.05  # 5% of total steps, adjust as needed
        #   param_names:
        #     - "sam_mask_decoder.*"
        - scheduler:
            _target_: fvcore.common.param_scheduler.CosineParamScheduler
            start_value: ${scratch.base_lr}
            end_value: ${divide:${scratch.base_lr},1}
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.05  # 0.05 # 0.1
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names:
            - '*bias*'
          module_cls_names: ['torch.nn.LayerNorm']

  loss:
    all:
      _target_: custom_models.models.vggtformer.criterion.SetCriterion
      num_classes: ${scratch.num_classes}
      weight_dict:
        # loss_mask: 1.0
        loss_mask: 5.0
        loss_dice: 5.0
      eos_coef: 0.1
      alpha: -1
      loss_weighting: log  # HEADS UP: log linear or default
      losses: ${scratch.losses}  # ['masks'] or ['pixelwise_mask']
      # losses: ['pixelwise_mask']  # Dont forget to change the weight dict too
      # losses: ['masks']  # Dont forget to change the weight dict too
      num_points: ${scratch.num_sampling_points}
      obj_ids_extra_sampling: ${scratch.obj_ids_extra_sampling}
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
      image_size: ${scratch.resolution}
      deep_supervision: False
      pointwise_mask: False
    val:
      _target_: custom_models.models.vggtformer.criterion.SetCriterion
      num_classes: ${scratch.num_classes}
      weight_dict:
        # loss_mask: 1.0
        loss_mask: 5.0
        loss_dice: 5.0
      eos_coef: 0.1
      alpha: -1
      loss_weighting: log  # HEADS UP: log linear or default
      losses: ${scratch.losses}  # ['masks'] or ['pixelwise_mask']
      # losses: ['pixelwise_mask']  # Dont forget to change the weight dict too
      # losses: ['masks']  # Dont forget to change the weight dict too
      num_points: ${scratch.num_sampling_points}
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
      image_size: ${scratch.resolution}
      deep_supervision: False
      pointwise_mask: False
  
  distributed:
    backend: nccl
    find_unused_parameters: False

  logging:
    tensorboard_writer:
      _target_: training.utils.logger.make_tensorboard_logger
      log_dir:  ${launcher.experiment_log_dir}/tensorboard
      flush_secs: 120
      should_log: True
    log_dir: ${launcher.experiment_log_dir}/logs
    log_freq: 10

  # initialize from a SAM 2 checkpoint
  checkpoint:
    # resume_from: sam2_logs/09_01_19_49/checkpoints/checkpoint_1.pt
    # resume_from: sam2_logs/09_14_07_30_VGGT/checkpoints/checkpoint_5.pt
    # resume_from: sam2_logs/09_04_15_01_VGGT/checkpoints/checkpoint_8.pt
    # resume_from: sam2_logs/09_23_11_38_VGGT/checkpoints/checkpoint_1.pt
    # resume_from: sam2_logs/09_24_14_29_VGGT/checkpoints/checkpoint_14.pt
    # resume_from: sam2_logs/09_24_08_50/checkpoints/checkpoint_12.pt
    # resume_from: sam2_logs/09_27_10_07/checkpoints/checkpoint_56.pt
    save_freq: 1 # 0 only last checkpoint is saved.
    save_dir: ${launcher.experiment_log_dir}/checkpoints
    model_weight_initializer:
      _partial_: True
      _target_: custom_models.models.sam2former.lib.load_state_dict_into_model_vggt
      strict: True
      use_dpt_weights: ${scratch.use_dpt_weights}
      pre_trained_for_seg: ${scratch.pre_trained_for_seg}
      state_dict:
        _target_: training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels
        checkpoint_path: /home/guests/tuna_gurbuz/prototype/models/vggt/checkpoints/model.pt
        # checkpoint_path: /home/guests/tuna_gurbuz/prototype/sam2_logs/09_14_07_30_VGGT/checkpoints/checkpoint_5.pt
        # checkpoint_path: /home/guests/tuna_gurbuz/prototype/sam2_logs/09_04_15_01_VGGT/checkpoints/checkpoint_8.pt
        # checkpoint_path: /home/guests/tuna_gurbuz/prototype/sam2_logs/09_18_16_17_VGGT/checkpoints/checkpoint_9.pt
        # checkpoint_path: /home/guests/tuna_gurbuz/prototype/sam2_logs/09_24_14_29_VGGT/checkpoints/checkpoint_14.pt
        # checkpoint_path: /home/guests/tuna_gurbuz/prototype/sam2_logs/09_23_11_38_VGGT/checkpoints/checkpoint_1.pt
        # checkpoint_path: /home/guests/tuna_gurbuz/prototype/sam2_logs/09_24_08_50/checkpoints/checkpoint_12.pt
        # checkpoint_path: /home/guests/tuna_gurbuz/prototype/sam2_logs/09_27_10_07/checkpoints/checkpoint_56.pt
        ckpt_state_dict_keys: []

launcher:
  num_nodes: 1
  gpus_per_node: 1
  experiment_log_dir: null # Path to log directory, defaults to ./sam2_logs/${config_name}

# SLURM args if running on a cluster
submitit:
  partition: null
  account: null
  qos: null
  cpus_per_task: 10
  use_cluster: false
  timeout_hour: 24
  name: null
  port_range: [10000, 65000]

